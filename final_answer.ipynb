{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure using only one GPU\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # Fixed from __main__\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_model(model_name=\"malhajar/meditron-7b-chat\"):\n",
    "    \"\"\"Load the medical model and tokenizer with 4-bit quantization and prepare for fine-tuning.\"\"\"\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # First load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True,\n",
    "        use_fast=True\n",
    "    )\n",
    "    \n",
    "    # Define prompt template for this specific model\n",
    "    if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
    "        logger.info(\"Setting chat template for Meditron\")\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'system' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}### Response:\n",
    "{{ message['content'] }}\n",
    "{% endif %}\n",
    "{% if loop.last and add_generation_prompt %}### Response:\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\"\n",
    "    \n",
    "    # Clear memory before loading model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Configure quantization with CPU offloading enabled\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization config and auto device map\n",
    "    logger.info(\"Loading 4-bit quantized model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # Simplified device mapping\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Return the base model and tokenizer without PEFT modifications\n",
    "    # We'll apply PEFT in a separate step\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_peft_to_model(model):\n",
    "    \"\"\"Apply Parameter-Efficient Fine-Tuning (PEFT) to the model.\"\"\"\n",
    "    logger.info(\"Applying PEFT to the model...\")\n",
    "    \n",
    "    # CRITICAL STEP 1: Properly prepare model for kbit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # CRITICAL STEP 2: Ensure input gradients are enabled\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    \n",
    "    # CRITICAL STEP 3: Define LoRA configuration with more comprehensive targets\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,                    # Rank for LoRA\n",
    "        lora_alpha=32,          # Alpha parameter for LoRA\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention modules\n",
    "        lora_dropout=0.05,     \n",
    "        bias=\"none\",           \n",
    "        task_type=\"CAUSAL_LM\"  \n",
    "    )\n",
    "    \n",
    "    # CRITICAL STEP 4: Apply LoRA to create a trainable model using get_peft_model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Verify trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    logger.info(f\"Trainable parameters: {trainable_params}\")\n",
    "    logger.info(f\"All parameters: {all_params}\")\n",
    "    logger.info(f\"Trainable%: {100 * trainable_params / all_params:.4f}%\")\n",
    "    \n",
    "    return peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_for_fine_tuning(csv_file):\n",
    "    \"\"\"Process the dataset for fine-tuning using original_question and ideal_answer columns.\"\"\"\n",
    "    logger.info(f\"Processing dataset from {csv_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        # Create a small test dataset for debugging\n",
    "        logger.info(\"Creating a small sample dataset for testing\")\n",
    "        test_data = {\n",
    "            'original_question': [\n",
    "                \"What are the symptoms of diabetes?\",\n",
    "                \"How is hypertension diagnosed?\",\n",
    "                \"What are common treatments for migraine?\"\n",
    "            ],\n",
    "            'ideal_answer': [\n",
    "                \"Common symptoms of diabetes include frequent urination, increased thirst, unexplained weight loss, extreme hunger, blurred vision, tingling in the extremities, and frequent infections.\",\n",
    "                \"Hypertension is diagnosed when blood pressure readings consistently show systolic pressure above 130 mmHg or diastolic pressure above 80 mmHg. Diagnosis typically requires multiple readings over time.\",\n",
    "                \"Common treatments for migraine include pain relievers, triptans, anti-nausea medications, preventive medications like beta blockers, and lifestyle changes such as stress management and regular sleep.\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'original_question' not in df.columns or 'ideal_answer' not in df.columns:\n",
    "        logger.error(\"Dataset missing required columns (original_question and ideal_answer)\")\n",
    "        return None\n",
    "    \n",
    "    # Filter rows that have both question and ideal answer\n",
    "    df = df.dropna(subset=['original_question', 'ideal_answer'])\n",
    "    \n",
    "    # For low memory, limit dataset size\n",
    "    if len(df) > 50:  # Reduced from 100 to 50\n",
    "        logger.info(f\"Limiting dataset to 50 examples for memory efficiency (from {len(df)})\")\n",
    "        df = df.sample(50, random_state=42)\n",
    "    \n",
    "    logger.info(f\"Dataset has {len(df)} valid training examples\")\n",
    "    \n",
    "    # Create a system prompt for all examples\n",
    "    system_prompt = \"You are an AI Medical Assistant. Give accurate and helpful answers to medical questions.\"\n",
    "    \n",
    "    # Create formatted examples for fine-tuning\n",
    "    train_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Format each example as a conversation with proper tokens\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": row['original_question']},\n",
    "            {\"role\": \"assistant\", \"content\": row['ideal_answer']}\n",
    "        ]\n",
    "        \n",
    "        # Format the conversation according to the model's expected format\n",
    "        example = {\"conversation\": conversation}\n",
    "        train_data.append(example)\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_for_fine_tuning(csv_file):\n",
    "    \"\"\"Process the dataset for fine-tuning using original_question and ideal_answer columns.\"\"\"\n",
    "    logger.info(f\"Processing dataset from {csv_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        # Create a small test dataset for debugging\n",
    "        logger.info(\"Creating a small sample dataset for testing\")\n",
    "        test_data = {\n",
    "            'original_question': [\n",
    "                \"What are the symptoms of diabetes?\",\n",
    "                \"How is hypertension diagnosed?\",\n",
    "                \"What are common treatments for migraine?\"\n",
    "            ],\n",
    "            'ideal_answer': [\n",
    "                \"Common symptoms of diabetes include frequent urination, increased thirst, unexplained weight loss, extreme hunger, blurred vision, tingling in the extremities, and frequent infections.\",\n",
    "                \"Hypertension is diagnosed when blood pressure readings consistently show systolic pressure above 130 mmHg or diastolic pressure above 80 mmHg. Diagnosis typically requires multiple readings over time.\",\n",
    "                \"Common treatments for migraine include pain relievers, triptans, anti-nausea medications, preventive medications like beta blockers, and lifestyle changes such as stress management and regular sleep.\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'original_question' not in df.columns or 'ideal_answer' not in df.columns:\n",
    "        logger.error(\"Dataset missing required columns (original_question and ideal_answer)\")\n",
    "        return None\n",
    "    \n",
    "    # Filter rows that have both question and ideal answer\n",
    "    df = df.dropna(subset=['original_question', 'ideal_answer'])\n",
    "    \n",
    "    # For low memory, limit dataset size\n",
    "    if len(df) > 50:  # Reduced from 100 to 50\n",
    "        logger.info(f\"Limiting dataset to 50 examples for memory efficiency (from {len(df)})\")\n",
    "        df = df.sample(50, random_state=42)\n",
    "    \n",
    "    logger.info(f\"Dataset has {len(df)} valid training examples\")\n",
    "    \n",
    "    # Create a system prompt for all examples\n",
    "    system_prompt = \"You are an AI Medical Assistant. Give accurate and helpful answers to medical questions.\"\n",
    "    \n",
    "    # Create formatted examples for fine-tuning\n",
    "    train_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Format each example as a conversation with proper tokens\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": row['original_question']},\n",
    "            {\"role\": \"assistant\", \"content\": row['ideal_answer']}\n",
    "        ]\n",
    "        \n",
    "        # Format the conversation according to the model's expected format\n",
    "        example = {\"conversation\": conversation}\n",
    "        train_data.append(example)\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataCollator:\n",
    "    def __init__(self, tokenizer, max_length=192):  # Reduced max length for memory savings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Extract conversations\n",
    "        conversations = [ex[\"conversation\"] for ex in examples]\n",
    "        \n",
    "        # Format according to the tokenizer's chat template if available\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "            input_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    conv[:-1],  # Exclude the assistant's response\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                for conv in conversations\n",
    "            ]\n",
    "            \n",
    "            target_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    conv,  # Full conversation\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "                for conv in conversations\n",
    "            ]\n",
    "        else:\n",
    "            # Fallback formatting\n",
    "            input_texts = []\n",
    "            target_texts = []\n",
    "            \n",
    "            for conv in conversations:\n",
    "                system = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"system\"), \"\")\n",
    "                user = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"user\"), \"\")\n",
    "                assistant = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"assistant\"), \"\")\n",
    "                \n",
    "                input_text = f\"### Instruction:\\n{system}\\n### Instruction:\\n{user}\\n### Response:\"\n",
    "                target_text = f\"### Instruction:\\n{system}\\n### Instruction:\\n{user}\\n### Response:\\n{assistant}\"\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        model_inputs = self.tokenizer(\n",
    "            input_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        labels = self.tokenizer(\n",
    "            target_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # Create labels, replacing padding tokens with -100\n",
    "        labels_with_ignore_index = labels.clone()\n",
    "        labels_with_ignore_index[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Replace padding in input portion with -100 for loss calculation\n",
    "        for i, (inp, full) in enumerate(zip(model_inputs[\"input_ids\"], labels)):\n",
    "            # Find end of input by comparing with full target\n",
    "            input_len = len(inp)\n",
    "            for j in range(min(input_len, len(full))):\n",
    "                if j < input_len:\n",
    "                    labels_with_ignore_index[i, j] = -100\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "        return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, tokenizer, dataset, output_dir=\"fine_tuned_model\"):\n",
    "    \"\"\"Fine-tune the model using LoRA.\"\"\"\n",
    "    logger.info(\"Starting fine-tuning process\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define training arguments optimized for 4GB VRAM\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,              # Minimal epochs\n",
    "        per_device_train_batch_size=1,   # Minimal batch size\n",
    "        gradient_accumulation_steps=16,  # Increased for stability with small batches\n",
    "        learning_rate=2e-4,              \n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,               # Shorter warmup to save time\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,              # Keep only the best model\n",
    "        fp16=True,                       # Use mixed precision\n",
    "        report_to=\"none\",                # Disable reporting to save memory\n",
    "        push_to_hub=False,\n",
    "        gradient_checkpointing=True,     # Enable gradient checkpointing\n",
    "        optim=\"paged_adamw_8bit\",        # Use 8-bit optimizer for memory savings\n",
    "        max_grad_norm=0.3,               # Reduce gradient norm for stability\n",
    "        dataloader_num_workers=0,        # No parallel loading\n",
    "        dataloader_pin_memory=False,     # Disable pinned memory\n",
    "        max_steps=50,                    # Limit training steps\n",
    "    )\n",
    "    \n",
    "    # Define trainer with the custom data collator\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=MedicalDataCollator(tokenizer, max_length=400),  # Reduced max length\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    logger.info(f\"Saving fine-tuned model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions_file(input_csv, output_csv, model, tokenizer, batch_size=8):\n",
    "    \"\"\"Process the questions file with the fine-tuned model.\"\"\"\n",
    "    import time\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"Loaded dataset with {len(df)} questions\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    answerer = MedicalQuestionAnswerer(model, tokenizer)\n",
    "\n",
    "    if 'original_answer' not in df.columns:\n",
    "        df['original_answer'] = \"\"\n",
    "    if 'faq_answer' not in df.columns:\n",
    "        df['faq_answer'] = \"\"\n",
    "\n",
    "    last_processed = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.notna(row['original_answer']) and row['original_answer'] != \"\":\n",
    "            last_processed = i\n",
    "\n",
    "    if last_processed > 0:\n",
    "        logger.info(f\"Resuming from question {last_processed+1}\")\n",
    "\n",
    "    save_interval = 50\n",
    "\n",
    "    # Using ThreadPoolExecutor for concurrent processing of questions in batches\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for i in range(last_processed, len(df), batch_size):\n",
    "            batch_end = min(i + batch_size, len(df))\n",
    "            batch_df = df.iloc[i:batch_end].copy()\n",
    "            \n",
    "            for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {i//batch_size + 1}/{(len(df)-last_processed)//batch_size + 1}\"):\n",
    "                if pd.notna(row['original_answer']) and row['original_answer'] != \"\":\n",
    "                    continue\n",
    "                \n",
    "                original_question = row['original_question']\n",
    "                generated_question = row['generated_question'] if 'generated_question' in row else None\n",
    "                \n",
    "                futures.append(executor.submit(answerer.answer_question, original_question))\n",
    "                \n",
    "                if generated_question:\n",
    "                    futures.append(executor.submit(answerer.answer_question, generated_question))\n",
    "                \n",
    "                if idx % save_interval == 0:\n",
    "                    df.to_csv(output_csv, index=False)\n",
    "\n",
    "            # Collect results from futures\n",
    "            result_idx = 0\n",
    "            for future in tqdm(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Update answers in dataframe - need to handle this better with specific indices\n",
    "                    if result_idx % 2 == 0:  # Even indices are original questions\n",
    "                        df.at[last_processed + result_idx//2, 'original_answer'] = result\n",
    "                    else:  # Odd indices are generated questions\n",
    "                        df.at[last_processed + result_idx//2, 'faq_answer'] = result\n",
    "                    result_idx += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing question: {e}\")\n",
    "\n",
    "            df.to_csv(output_csv, index=False)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            questions_processed = batch_end - last_processed\n",
    "            avg_time_per_q = elapsed / max(1, questions_processed)\n",
    "            remaining_qs = len(df) - batch_end\n",
    "            \n",
    "            est_time_remaining = avg_time_per_q * remaining_qs\n",
    "            \n",
    "            logger.info(f\"Processed {batch_end}/{len(df)} questions. \"\n",
    "                        f\"Avg: {avg_time_per_q:.2f}s per question. \"\n",
    "                        f\"Est. remaining: {est_time_remaining/60:.1f} minutes\")\n",
    "\n",
    "            # Clear GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            futures = []  # Reset futures for the next batch\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed in {total_time/60:.1f} minutes. Generated answers saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalQuestionAnswerer:\n",
    "    \"\"\"Class to answer medical questions using a medical model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        \"\"\"Initialize with pre-loaded model and tokenizer.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Get the device where the model is located\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        # Set up model for inference\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Pre-compile prompt template for speed\n",
    "        self.sys_message = \"You are an AI Medical Assistant. Give brief answers as a medical professional.\"\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Generate an answer for a given question.\"\"\"\n",
    "        try:\n",
    "            # Clear cache before inference\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Check if apply_chat_template is available\n",
    "            if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": self.sys_message},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ]\n",
    "                prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            else:\n",
    "                # Fallback to manual formatting\n",
    "                prompt = f\"<|system|>\\n{self.sys_message}\\n</s>\\n<|user|>\\n{question}\\n</s>\\n<|assistant|>\"\n",
    "            \n",
    "            # Tokenize directly to the correct device\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "           \n",
    "            with torch.no_grad():\n",
    "                # Use memory-efficient generation settings\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,   # Reduced token count\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,      \n",
    "                    use_cache=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Extract the generated text\n",
    "            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the assistant's response\n",
    "            if \"<|assistant|>\" in response_text:\n",
    "                answer = response_text.split(\"<|assistant|>\")[-1].strip()\n",
    "            elif \"### Response:\" in response_text:\n",
    "                answer = response_text.split(\"### Response:\")[-1].strip()\n",
    "            else:\n",
    "                answer = response_text.split(prompt)[-1].strip()\n",
    "            \n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {str(e)}\")\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "        finally:\n",
    "            # Clean up memory after generation\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions_file(input_csv, output_csv, model, tokenizer, batch_size=1):  # Single question at a time\n",
    "    \"\"\"Process the questions file with the fine-tuned model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"Loaded dataset with {len(df)} questions\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    answerer = MedicalQuestionAnswerer(model, tokenizer)\n",
    "\n",
    "    if 'original_answer' not in df.columns:\n",
    "        df['original_answer'] = \"\"\n",
    "    if 'faq_answer' not in df.columns:\n",
    "        df['faq_answer'] = \"\"\n",
    "\n",
    "    last_processed = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.notna(row['original_answer']) and row['original_answer'] != \"\":\n",
    "            last_processed = i\n",
    "\n",
    "    if last_processed > 0:\n",
    "        logger.info(f\"Resuming from question {last_processed+1}\")\n",
    "\n",
    "    save_interval = 2  # Save more frequently\n",
    "    \n",
    "    # Process questions in smaller batches (single-threaded for stability)\n",
    "    for i in range(last_processed, len(df), batch_size):\n",
    "        batch_end = min(i + batch_size, len(df))\n",
    "        batch_df = df.iloc[i:batch_end].copy()\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Process original question if not already answered\n",
    "                if not pd.notna(row['original_answer']) or row['original_answer'] == \"\":\n",
    "                    original_question = row['original_question']\n",
    "                    answer = answerer.answer_question(original_question)\n",
    "                    df.at[idx, 'original_answer'] = answer\n",
    "                    logger.info(f\"Processed question {idx}\")\n",
    "                \n",
    "                # Process generated question if available and not already answered\n",
    "                if 'generated_question' in row and pd.notna(row['generated_question']):\n",
    "                    if not pd.notna(row['faq_answer']) or row['faq_answer'] == \"\":\n",
    "                        generated_question = row['generated_question']\n",
    "                        faq_answer = answerer.answer_question(generated_question)\n",
    "                        df.at[idx, 'faq_answer'] = faq_answer\n",
    "                \n",
    "                # Save progress frequently\n",
    "                if idx % save_interval == 0:\n",
    "                    df.to_csv(output_csv, index=False)\n",
    "                    logger.info(f\"Saved progress at index {idx}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing question {idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save progress after each batch\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        questions_processed = batch_end - last_processed\n",
    "        avg_time_per_q = elapsed / max(1, questions_processed)\n",
    "        remaining_qs = len(df) - batch_end\n",
    "        \n",
    "        est_time_remaining = avg_time_per_q * remaining_qs\n",
    "        \n",
    "        logger.info(f\"Processed {batch_end}/{len(df)} questions. \"\n",
    "                    f\"Avg: {avg_time_per_q:.2f}s per question. \"\n",
    "                    f\"Est. remaining: {est_time_remaining/60:.1f} minutes\")\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed in {total_time/60:.1f} minutes. Generated answers saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model: malhajar/meditron-7b-chat\n",
      "INFO:__main__:Setting chat template for Meditron\n",
      "WARNING:__main__:Failed to load 7B model: No package metadata was found for bitsandbytes\n",
      "INFO:__main__:Falling back to smaller model...\n",
      "INFO:__main__:Loading model: malhajar/meditron-3b-chat\n",
      "ERROR:__main__:An error occurred in the main process: malhajar/meditron-3b-chat is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_177421/1019782692.py\", line 22, in main\n",
      "    model, tokenizer = load_medical_model(base_model_name)\n",
      "  File \"/tmp/ipykernel_177421/4154498051.py\", line 32, in load_medical_model\n",
      "    quantization_config = BitsAndBytesConfig(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 433, in __init__\n",
      "    self.post_init()\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 491, in post_init\n",
      "    if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n",
      "  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
      "    return distribution(distribution_name).version\n",
      "  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
      "    return Distribution.from_name(distribution_name)\n",
      "  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
      "    raise PackageNotFoundError(name)\n",
      "importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/malhajar/meditron-3b-chat/resolve/main/tokenizer_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 342, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 860, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 967, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1482, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1374, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1294, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 278, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 302, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 454, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6860ee65-7097af695f1f7ab26067e8c0;c89d7af1-e71f-4c3e-abd2-78852d0b090b)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/malhajar/meditron-3b-chat/resolve/main/tokenizer_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_177421/1019782692.py\", line 26, in main\n",
      "    model, tokenizer = load_medical_model(\"malhajar/meditron-3b-chat\")\n",
      "  File \"/tmp/ipykernel_177421/4154498051.py\", line 6, in load_medical_model\n",
      "    tokenizer = AutoTokenizer.from_pretrained(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 881, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 713, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/home/vjti/.local/lib/python3.10/site-packages/transformers/utils/hub.py\", line 365, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: malhajar/meditron-3b-chat is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Import necessary libraries at the beginning\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "    \n",
    "    input_file = \"T5_FAQS1.csv\"  \n",
    "    output_file = \"medical_answers_finetuned.csv\"\n",
    "    fine_tuned_model_dir = \"fine_tuned_medical_model\"\n",
    "    \n",
    "    # Create offload directory\n",
    "    os.makedirs(\"offload_folder\", exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Clear any existing cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Load the base model and tokenizer\n",
    "        base_model_name = \"malhajar/meditron-7b-chat\"\n",
    "        try:\n",
    "            model, tokenizer = load_medical_model(base_model_name)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load 7B model: {e}\")\n",
    "            logger.info(\"Falling back to smaller model...\")\n",
    "            model, tokenizer = load_medical_model(\"malhajar/meditron-3b-chat\")\n",
    "        \n",
    "        # Process dataset for fine-tuning\n",
    "        dataset = process_dataset_for_fine_tuning(input_file)\n",
    "        \n",
    "        if dataset:\n",
    "            # Apply PEFT to the model - THIS IS THE KEY CHANGE\n",
    "            model = apply_peft_to_model(model)\n",
    "            \n",
    "            # Fine-tune the model with PEFT applied\n",
    "            model, tokenizer = fine_tune_model(model, tokenizer, dataset, fine_tuned_model_dir)\n",
    "            \n",
    "            # Free up memory before inference\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Process questions using the fine-tuned model  \n",
    "            process_questions_file(input_file, output_file, model, tokenizer)\n",
    "        else:\n",
    "            logger.error(\"Could not prepare dataset for fine-tuning. Check if the required columns exist.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main process: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CUDA not available. Using device: CPU\n",
      "INFO:__main__:--- Starting Medical FAQ Fine-Tuning and Processing Script ---\n",
      "INFO:__main__:Attempting to load base model: malhajar/meditron-7b-chat\n",
      "INFO:__main__:Tokenizer loaded successfully.\n",
      "INFO:__main__:Setting chat template for Meditron model.\n",
      "WARNING:__main__:Tokenizer does not have a pad token. Setting to eos_token.\n",
      "CRITICAL:__main__:Failed to load base model 'malhajar/meditron-7b-chat'. Cannot proceed with fine-tuning or inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union\n",
    "import psutil # Import psutil to check system RAM\n",
    "import traceback # For detailed error logging\n",
    "\n",
    "# Try importing necessary libraries early\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        BitsAndBytesConfig,\n",
    "        DataCollatorForSeq2Seq # Keep import, though using custom collator\n",
    "    )\n",
    "    # Import PeftModel explicitly for type checking\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "    import accelerate # Ensure accelerate is available\n",
    "    import bitsandbytes # Ensure bitsandbytes is available\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}\")\n",
    "    print(\"Please ensure transformers, peft, datasets, accelerate, bitsandbytes, and psutil are installed.\")\n",
    "    exit()\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Control GPU visibility if needed\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.info(f\"CUDA available. Using device: {device}\")\n",
    "    try:\n",
    "        gpu_index = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "        logger.info(f\"CUDA device name: {gpu_name}\")\n",
    "        t = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        r = torch.cuda.memory_reserved(gpu_index)\n",
    "        a = torch.cuda.memory_allocated(gpu_index)\n",
    "        f = r - a\n",
    "        logger.info(f\"Initial GPU Memory (Bytes): Total={t}, Reserved={r}, Allocated={a}\")\n",
    "        logger.info(f\"Initial GPU Memory (GB): Total={t/1e9:.2f}GB, Reserved={r/1e9:.2f}GB, Allocated={a/1e9:.2f}GB, FreeReserved={f/1e9:.2f}GB\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not get GPU details: {e}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    logger.info(\"CUDA not available. Using device: CPU\")\n",
    "\n",
    "# Global configuration\n",
    "INPUT_CSV = \"T5_FAQS1.csv\"\n",
    "OUTPUT_CSV = \"medical_answers_finetuned_v5.csv\" # Incremented version\n",
    "FINE_TUNED_MODEL_DIR = \"fine_tuned_medical_model_v5\" # Incremented version\n",
    "BASE_MODEL_NAME = \"malhajar/meditron-7b-chat\"\n",
    "MAX_DATASET_EXAMPLES = 50 # Limit examples for faster testing/demo\n",
    "MAX_TRAINING_STEPS = 50   # Limit training steps for faster testing/demo\n",
    "MAX_SEQ_LENGTH_COLLATOR = 128 # Max length for sequences during training (affects memory)\n",
    "MAX_SEQ_LENGTH_INFERENCE = 500 # Max length for input sequence during inference\n",
    "INFERENCE_BATCH_SIZE = 1 # Process one question at a time for inference\n",
    "SAVE_INTERVAL_INFERENCE = 5 # Save progress every N questions during inference\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clears GPU memory.\"\"\"\n",
    "    logger.debug(\"Clearing GPU Cache...\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        logger.debug(\"GPU Cache Cleared.\")\n",
    "    else:\n",
    "        logger.debug(\"No GPU available, skipping cache clearing.\")\n",
    "\n",
    "def load_base_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads the base quantized model and tokenizer with explicit memory limits.\"\"\"\n",
    "    logger.info(f\"Attempting to load base model: {model_name}\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 1. Load Tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        logger.info(\"Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load tokenizer for {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2. Set Chat Template & Padding Token\n",
    "    if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
    "        logger.info(\"Setting chat template for Meditron model.\")\n",
    "        # Basic template structure - adjust if needed based on model card\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}### Response:\n",
    "{{ message['content'] }}\n",
    "{% endif %}{% if loop.last and add_generation_prompt %}### Response:\n",
    "{% endif %}{% endfor %}\"\"\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        logger.warning(\"Tokenizer does not have a pad token. Setting to eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id # Explicitly set ID\n",
    "\n",
    "    # 3. Configure Quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    logger.info(f\"Using quantization config: {quantization_config}\")\n",
    "\n",
    "    # 4. Define Memory Limits (Using INTEGER key for GPU)\n",
    "    max_memory = {}\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_index = 0 # Assuming device 0\n",
    "        total_vram_bytes = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        # Leave ~2GB buffer for safety, adjust if needed\n",
    "        gpu_mem_limit_bytes = total_vram_bytes - int(2 * 1024**3)\n",
    "        max_memory[gpu_index] = f\"{gpu_mem_limit_bytes // (1024**2)}MiB\" # Use integer index\n",
    "        logger.info(f\"Calculated GPU memory limit for device {gpu_index}: {max_memory[gpu_index]}\")\n",
    "\n",
    "    total_ram_bytes = psutil.virtual_memory().total\n",
    "    # Limit CPU RAM usage to 80% to avoid system freeze\n",
    "    cpu_mem_limit_bytes = int(total_ram_bytes * 0.80)\n",
    "    max_memory['cpu'] = f\"{cpu_mem_limit_bytes // (1024**2)}MiB\"\n",
    "\n",
    "    logger.info(f\"Setting max_memory for accelerate: {max_memory}\")\n",
    "\n",
    "    # 5. Load Model\n",
    "    try:\n",
    "        logger.info(\"Loading 4-bit quantized model with device_map='auto' and max_memory...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\", # Automatically distribute model layers across devices\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.float16, # Use float16 for memory efficiency\n",
    "            low_cpu_mem_usage=True, # Try to load shards sequentially to save CPU RAM\n",
    "            max_memory=max_memory, # Apply memory limits per device\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        logger.info(\"Base model loaded successfully.\")\n",
    "        logger.info(f\"Model device map: {model.hf_device_map}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load base model {model_name} even with max_memory: {e}\")\n",
    "        logger.error(traceback.format_exc()) # Log full traceback\n",
    "        raise\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_peft_to_model(model, tokenizer):\n",
    "    \"\"\"Applies PEFT/LoRA adapters to the loaded base model.\"\"\"\n",
    "    logger.info(\"Applying PEFT/LoRA adapters to the model...\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 1. Prepare model for k-bit training\n",
    "    logger.info(\"Preparing model for k-bit training...\")\n",
    "    # Ensure gradient checkpointing is enabled here if desired for training\n",
    "    try:\n",
    "         # use_gradient_checkpointing=True can sometimes cause issues, let TrainingArguments handle it primarily.\n",
    "         # Set it to False here, rely on TrainingArguments.gradient_checkpointing=True\n",
    "         model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "         logger.info(\"prepare_model_for_kbit_training successful.\")\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Error during prepare_model_for_kbit_training: {e}\")\n",
    "         raise\n",
    "\n",
    "    # 2. Ensure input embeddings require gradients (can be crucial for some models/setups)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        logger.info(\"Enabling input require grads using model.enable_input_require_grads().\")\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        # Fallback method if the direct function isn't available\n",
    "        logger.info(\"Attempting to enable input require grads using forward hook.\")\n",
    "        try:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                 if isinstance(output, torch.Tensor) and output.is_floating_point():\n",
    "                     output.requires_grad_(True)\n",
    "            embed_module = model.get_input_embeddings()\n",
    "            if embed_module:\n",
    "                 embed_module.register_forward_hook(make_inputs_require_grad)\n",
    "                 logger.info(\"Gradient hook attached to input embeddings.\")\n",
    "            else:\n",
    "                 logger.warning(\"Could not find input embeddings module.\")\n",
    "        except Exception as e:\n",
    "             logger.warning(f\"Failed to attach gradient hook: {e}. This might be okay.\")\n",
    "\n",
    "    # 3. Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Rank of the update matrices (lower value = fewer parameters)\n",
    "        lora_alpha=16, # LoRA scaling factor\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Modules to apply LoRA to (check model architecture if needed)\n",
    "        lora_dropout=0.05, # Dropout probability for LoRA layers\n",
    "        bias=\"none\", # Whether to train biases ('none', 'all', or 'lora_only')\n",
    "        task_type=\"CAUSAL_LM\" # Task type for PEFT\n",
    "    )\n",
    "    logger.info(f\"Using LoRA config: {lora_config}\")\n",
    "\n",
    "    # 4. Apply LoRA using get_peft_model\n",
    "    try:\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        logger.info(\"PEFT model created successfully using get_peft_model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to apply PEFT to the model using get_peft_model: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 5. Verify trainable parameters\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # 6. <<< ADDED STEP: Explicitly mark as PEFT model >>>\n",
    "    # This might help older/buggy Trainer checks, although usually not needed.\n",
    "    if not isinstance(peft_model, PeftModel):\n",
    "         logger.warning(\"get_peft_model did not return a PeftModel instance!\")\n",
    "    else:\n",
    "         logger.info(\"Model is instance of PeftModel. Setting is_peft_model=True attribute just in case.\")\n",
    "         # Use setattr for safety in case attribute doesn't exist on all versions\n",
    "         setattr(peft_model, 'is_peft_model', True)\n",
    "\n",
    "    logger.info(f\"Model type after get_peft_model: {type(peft_model)}\")\n",
    "    return peft_model\n",
    "\n",
    "def process_dataset_for_fine_tuning(csv_file):\n",
    "    \"\"\"Loads and formats the dataset for fine-tuning.\"\"\"\n",
    "    logger.info(f\"Processing dataset from {csv_file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        logger.info(f\"Loaded dataframe with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Dataset file not found: {csv_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['original_question', 'ideal_answer']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logger.error(f\"Dataset missing required columns: {required_cols}. Found: {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Drop rows where essential columns are missing\n",
    "    df.dropna(subset=required_cols, inplace=True)\n",
    "    logger.info(f\"Rows after dropping NA in required columns: {len(df)}\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.error(\"No valid data remaining after filtering.\")\n",
    "        return None\n",
    "\n",
    "    # Limit dataset size if configured\n",
    "    if len(df) > MAX_DATASET_EXAMPLES:\n",
    "        logger.info(f\"Limiting dataset to {MAX_DATASET_EXAMPLES} examples (from {len(df)})\")\n",
    "        df = df.sample(n=MAX_DATASET_EXAMPLES, random_state=42) # Use sampling with fixed state\n",
    "\n",
    "    logger.info(f\"Using {len(df)} examples for fine-tuning.\")\n",
    "\n",
    "    # Define the system prompt\n",
    "    system_prompt = \"You are an AI Medical Assistant. Provide accurate and concise answers to medical questions based on the context provided.\"\n",
    "\n",
    "    # Format data into conversation structure\n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = str(row['original_question']).strip()\n",
    "        answer = str(row['ideal_answer']).strip()\n",
    "        if not question or not answer: # Skip rows with empty question or answer\n",
    "            continue\n",
    "        # Structure expected by tokenizer.apply_chat_template\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer} # The expected completion\n",
    "        ]\n",
    "        formatted_data.append({\"conversation\": conversation})\n",
    "\n",
    "    if not formatted_data:\n",
    "         logger.error(\"No valid conversation pairs found after formatting.\")\n",
    "         return None\n",
    "\n",
    "    # Create Hugging Face Dataset object\n",
    "    try:\n",
    "        dataset = Dataset.from_list(formatted_data)\n",
    "        logger.info(\"HuggingFace Dataset created successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create HuggingFace Dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class MedicalChatDataCollator:\n",
    "    \"\"\"Formats conversations and prepares inputs/labels for Causal LM fine-tuning.\"\"\"\n",
    "    def __init__(self, tokenizer, max_length=MAX_SEQ_LENGTH_COLLATOR):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        if not hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "             raise ValueError(\"Tokenizer must have `apply_chat_template` method.\")\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            logger.warning(\"Collator: Tokenizer pad_token_id is None. Using eos_token_id.\")\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[Dict[str, str]]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch_conversations = [ex[\"conversation\"] for ex in examples]\n",
    "\n",
    "        try:\n",
    "             # Prepare Inputs (prompt part only: system + user)\n",
    "             # We add `add_generation_prompt=True` which typically adds the assistant prompt start (e.g., \"### Response:\")\n",
    "             input_formatted = [\n",
    "                  self.tokenizer.apply_chat_template(conv[:-1], tokenize=False, add_generation_prompt=True)\n",
    "                  for conv in batch_conversations\n",
    "             ]\n",
    "             # Tokenize the inputs\n",
    "             model_inputs = self.tokenizer(\n",
    "                  input_formatted,\n",
    "                  max_length=self.max_length,\n",
    "                  padding=\"max_length\", # Pad to max_length\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\"\n",
    "             )\n",
    "\n",
    "             # Prepare Labels (full conversation: system + user + assistant)\n",
    "             # We do NOT add generation prompt here, as we want the full sequence including the assistant's answer.\n",
    "             labels_formatted = [\n",
    "                  self.tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n",
    "                  for conv in batch_conversations\n",
    "             ]\n",
    "             # Tokenize the full conversation to get the target labels\n",
    "             labels = self.tokenizer(\n",
    "                  labels_formatted,\n",
    "                  max_length=self.max_length,\n",
    "                  padding=\"max_length\", # Pad to max_length\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\"\n",
    "             )[\"input_ids\"]\n",
    "\n",
    "             # --- Label Masking ---\n",
    "             # We only want to compute loss on the assistant's response tokens.\n",
    "             # Mask tokens belonging to the system prompt, user query, and padding.\n",
    "             masked_labels = labels.clone()\n",
    "\n",
    "             for i in range(len(model_inputs[\"input_ids\"])):\n",
    "                  # Calculate the length of the input prompt (system + user + assistant prompt start)\n",
    "                  # Use attention_mask sum, as input_ids might contain padding AFTER truncation\n",
    "                  input_ids_len = model_inputs[\"attention_mask\"][i].sum().item()\n",
    "\n",
    "                  # Mask all tokens up to the end of the input prompt\n",
    "                  masked_labels[i, :input_ids_len] = -100 # -100 is the standard ignore index for loss calculation\n",
    "\n",
    "                  # Also mask padding tokens in the labels\n",
    "                  # Find indices where labels are the padding token ID\n",
    "                  label_pad_indices = (labels[i] == self.tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "                  if len(label_pad_indices) > 0:\n",
    "                      # Mask all padding tokens (usually at the end)\n",
    "                      # Get the index of the first padding token\n",
    "                      first_pad_index = label_pad_indices[0].item()\n",
    "                      masked_labels[i, first_pad_index:] = -100\n",
    "\n",
    "             # Check if any example ended up with all labels masked (problematic)\n",
    "             if torch.all(masked_labels == -100, dim=1).any():\n",
    "                  logger.warning(\"Warning: An example has all labels masked. This might indicate issues with sequence lengths, truncation, or the collator logic.\")\n",
    "                  # Consider adding more detailed logging here if this happens frequently\n",
    "\n",
    "             model_inputs[\"labels\"] = masked_labels\n",
    "             return model_inputs\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error in Data Collator: {e}\")\n",
    "             logger.error(traceback.format_exc())\n",
    "             # Return empty batch or raise error? Returning empty might cause issues later.\n",
    "             raise # Let the error propagate\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, output_dir):\n",
    "    \"\"\"Fine-tunes the PEFT model using the Trainer API.\"\"\"\n",
    "    logger.info(\"Starting fine-tuning process...\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1, # Single epoch for quick demo/test\n",
    "        per_device_train_batch_size=1, # Small batch size due to model size/memory\n",
    "        gradient_accumulation_steps=16, # Accumulate gradients to simulate larger batch size (effective batch size = 1 * 16 = 16)\n",
    "        learning_rate=2e-4, # Common learning rate for LoRA\n",
    "        weight_decay=0.01, # Regularization\n",
    "        warmup_ratio=0.03, # Warmup steps as a fraction of total steps\n",
    "        logging_steps=5, # Log metrics every 5 steps\n",
    "        save_strategy=\"steps\", # Save checkpoints based on steps\n",
    "        save_steps=max(1, MAX_TRAINING_STEPS // 2), # Save halfway through (adjust as needed)\n",
    "        save_total_limit=1, # Keep only the latest checkpoint\n",
    "        fp16=torch.cuda.is_available(), # Enable mixed-precision training if CUDA is available\n",
    "        gradient_checkpointing=True, # Use gradient checkpointing to save memory (at cost of compute) - RELY ON THIS\n",
    "        optim=\"paged_adamw_8bit\", # Use paged AdamW optimizer for memory efficiency with QLoRA\n",
    "        max_grad_norm=0.3, # Gradient clipping\n",
    "        dataloader_num_workers=0, # Set to 0 or small number, can cause issues otherwise\n",
    "        dataloader_pin_memory=False, # Often False is better with device_map='auto'\n",
    "        max_steps=MAX_TRAINING_STEPS, # Limit total training steps\n",
    "        report_to=\"none\", # Disable external reporting (like wandb) for simplicity\n",
    "        push_to_hub=False, # Don't push to Hugging Face Hub\n",
    "        remove_unused_columns=True, # Let Trainer remove columns not used by the model\n",
    "        # Added for potential stability\n",
    "        # ddp_find_unused_parameters=False # Sometimes needed with PEFT/gradient checkpointing\n",
    "    )\n",
    "    logger.info(f\"Using Training Arguments: {training_args}\")\n",
    "\n",
    "    # Data Collator Instance\n",
    "    data_collator = MedicalChatDataCollator(tokenizer, max_length=MAX_SEQ_LENGTH_COLLATOR)\n",
    "\n",
    "    # === Debugging Logs Before Trainer Init ===\n",
    "    logger.info(f\"--- Preparing to initialize Trainer ---\")\n",
    "    logger.info(f\"Model object ID: {id(model)}\")\n",
    "    logger.info(f\"Model type passed to fine_tune_model: {type(model)}\")\n",
    "    logger.info(f\"Is model instance of PeftModel? {isinstance(model, PeftModel)}\")\n",
    "    logger.info(f\"Does model have 'peft_config' attribute? {hasattr(model, 'peft_config')}\")\n",
    "    logger.info(f\"Does model have 'is_peft_model' attribute set? {getattr(model, 'is_peft_model', 'Not Set')}\")\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "         logger.info(f\"Model device map: {model.hf_device_map}\")\n",
    "    else:\n",
    "         logger.info(\"Model does not have 'hf_device_map' attribute.\")\n",
    "    if hasattr(model, 'is_quantized'):\n",
    "         logger.info(f\"Model is_quantized: {model.is_quantized}\")\n",
    "    else:\n",
    "         logger.info(\"Model does not have 'is_quantized' attribute.\")\n",
    "    # Check base model properties if it's a PeftModel\n",
    "    if isinstance(model, PeftModel) and hasattr(model, 'base_model'):\n",
    "         base = model.base_model\n",
    "         logger.info(f\"Base model type: {type(base)}\")\n",
    "         logger.info(f\"Base model is_quantized: {getattr(base, 'is_quantized', 'Not Set')}\")\n",
    "         logger.info(f\"Base model has quantization_config: {hasattr(base.config, 'quantization_config')}\")\n",
    "         if hasattr(base.config, 'quantization_config'):\n",
    "              logger.info(f\"Base model quantization_config type: {type(base.config.quantization_config)}\")\n",
    "    logger.info(f\"--- End Pre-Trainer Init Logs ---\")\n",
    "    # === End Debugging Logs ===\n",
    "\n",
    "    # Trainer Initialization\n",
    "    trainer = None # Initialize to None\n",
    "    try:\n",
    "        trainer = Trainer(\n",
    "            model=model, # Should be the PeftModel instance from apply_peft_to_model\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            # callbacks=[...] # Add custom callbacks if needed\n",
    "        )\n",
    "        logger.info(\"Trainer initialized successfully.\")\n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"ValueError during Trainer initialization: {ve}\")\n",
    "        logger.error(\"This likely means the Trainer still doesn't recognize the model as PEFT-compatible, possibly due to issues with quantization or PEFT setup.\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, tokenizer # Cannot proceed if Trainer fails\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during Trainer initialization: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, tokenizer\n",
    "\n",
    "    # Training\n",
    "    logger.info(\"Starting training...\")\n",
    "    trained_model = None\n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = trainer.train()\n",
    "        logger.info(\"Training completed.\")\n",
    "        # Log metrics\n",
    "        metrics = train_result.metrics\n",
    "        logger.info(f\"Training metrics: {metrics}\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trained_model = model # Assign model if training successful\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        # Attempt to save state even if training failed mid-way\n",
    "        try:\n",
    "            logger.warning(\"Attempting to save model state after training error...\")\n",
    "            save_path = f\"{output_dir}/error_save\"\n",
    "            if trainer is not None:\n",
    "                 trainer.save_model(output_dir=save_path)\n",
    "                 tokenizer.save_pretrained(save_path)\n",
    "                 logger.info(f\"Model state saved to {save_path}\")\n",
    "            else:\n",
    "                 logger.error(\"Trainer was not initialized, cannot save model.\")\n",
    "        except Exception as save_e:\n",
    "            logger.error(f\"Could not save model after error: {save_e}\")\n",
    "        # Return None for model if training failed critically\n",
    "        return None, tokenizer\n",
    "\n",
    "    # Save final model adapters (LoRA weights) and tokenizer\n",
    "    if trained_model is not None:\n",
    "        logger.info(f\"Saving fine-tuned PEFT adapters and tokenizer to {output_dir}\")\n",
    "        try:\n",
    "            # save_model() with PEFT model saves only the adapters by default\n",
    "            trainer.save_model(output_dir)\n",
    "            # Save the tokenizer configuration as well\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            logger.info(\"Model adapters and tokenizer saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving final model/tokenizer: {e}\")\n",
    "            pass # Return the model object anyway, even if saving failed\n",
    "    else:\n",
    "         logger.warning(\"Training did not complete successfully, final model not saved via trainer.save_model.\")\n",
    "\n",
    "    # Cleanup\n",
    "    del trainer\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return trained_model, tokenizer # Return the model (potentially with trained adapters)\n",
    "\n",
    "class MedicalQuestionAnswerer:\n",
    "    \"\"\"Generates answers using the fine-tuned PEFT model.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        try:\n",
    "             # If using device_map, the device might be complex, find a parameter's device\n",
    "             self.device = next(model.parameters()).device\n",
    "        except Exception:\n",
    "             logger.warning(\"Could not automatically determine model device. Assuming CPU or first CUDA device if available.\")\n",
    "             self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        self.sys_message = \"You are an AI Medical Assistant. Give concise and accurate answers.\" # System prompt for inference\n",
    "        logger.info(f\"Question Answerer initialized on device: {self.device}\")\n",
    "\n",
    "        # Ensure pad token is set for generation\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    @torch.inference_mode() # Disable gradient calculations for inference\n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Generates an answer for a single question.\"\"\"\n",
    "        if not question or not isinstance(question, str):\n",
    "             logger.warning(f\"Invalid question provided: '{question}'. Skipping.\")\n",
    "             return \"Error: Invalid question provided.\"\n",
    "\n",
    "        logger.debug(f\"Answering question: '{question[:50]}...'\")\n",
    "        clear_gpu_memory() # Clear cache before generating\n",
    "\n",
    "        try:\n",
    "            # Prepare input using the chat template\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.sys_message},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True, # Add the prompt for the assistant's turn\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device) # Move inputs to the model's device\n",
    "\n",
    "            input_length = inputs.shape[1]\n",
    "\n",
    "            # Optional: Check if input exceeds inference length limit\n",
    "            if input_length >= MAX_SEQ_LENGTH_INFERENCE:\n",
    "                 logger.warning(f\"Input sequence length ({input_length}) is >= MAX_SEQ_LENGTH_INFERENCE ({MAX_SEQ_LENGTH_INFERENCE}). Input might be truncated by model implicitly or cause issues.\")\n",
    "\n",
    "            # --- Generate the answer ---\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs,\n",
    "                # *** MODIFIED PARAMETER FOR LONGER ANSWERS ***\n",
    "                max_new_tokens=400, # Increased from 50 to allow for ~7-8 sentences\n",
    "                temperature=0.6,    # Controls randomness. Lower is more deterministic.\n",
    "                top_p=0.9,          # Nucleus sampling parameter\n",
    "                do_sample=True,     # Enable sampling strategies (temperature, top_p)\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id, # Ensure generation stops at EOS\n",
    "                use_cache=True      # Speed up generation\n",
    "                # Add other generation parameters if needed (e.g., repetition_penalty)\n",
    "            )\n",
    "\n",
    "            # Decode only the newly generated tokens\n",
    "            output_tokens = outputs[0, input_length:]\n",
    "            answer = self.tokenizer.decode(output_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "            logger.debug(f\"Generated answer: '{answer[:100]}...'\")\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer for question '{question[:50]}...': {str(e)}\")\n",
    "            logger.debug(traceback.format_exc()) # Log traceback for debugging\n",
    "            return f\"Error: Processing failed during generation.\"\n",
    "        finally:\n",
    "            clear_gpu_memory() # Clear cache after generating\n",
    "\n",
    "def process_questions_file(input_csv, output_csv, model, tokenizer):\n",
    "    \"\"\"Processes questions from CSV sequentially with resuming and saving.\"\"\"\n",
    "    logger.info(f\"Starting sequential question processing from {input_csv}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"Loaded {len(df)} questions from {input_csv}.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input CSV not found: {input_csv}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading input CSV {input_csv}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize the question answerer class\n",
    "    answerer = MedicalQuestionAnswerer(model, tokenizer)\n",
    "\n",
    "    # Ensure output columns exist, initialize with NA if not\n",
    "    if 'original_answer' not in df.columns: df['original_answer'] = pd.NA\n",
    "    # Check if 'faq_answer' should exist based on input columns\n",
    "    if 'generated_question' in df.columns and 'faq_answer' not in df.columns:\n",
    "        df['faq_answer'] = pd.NA\n",
    "\n",
    "    # --- Resuming Logic ---\n",
    "    last_processed_index = -1\n",
    "    if os.path.exists(output_csv):\n",
    "        logger.info(f\"Output file {output_csv} found. Attempting to resume.\")\n",
    "        try:\n",
    "            df_existing = pd.read_csv(output_csv)\n",
    "            # Find the last row where 'original_answer' is NOT NA\n",
    "            valid_indices = df_existing['original_answer'].dropna().index\n",
    "            if not valid_indices.empty:\n",
    "                 last_processed_index = valid_indices[-1]\n",
    "                 logger.info(f\"Resuming from index {last_processed_index + 1}.\")\n",
    "                 # Update the current dataframe with already processed answers from the existing file\n",
    "                 # Only update up to the last processed index to avoid overwriting potentially newer data\n",
    "                 df.update(df_existing.iloc[:last_processed_index+1])\n",
    "            else:\n",
    "                logger.info(\"No previously processed answers found in output file, starting fresh.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "             logger.warning(f\"Output file {output_csv} is empty. Starting fresh.\")\n",
    "             last_processed_index = -1\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read or parse existing output file {output_csv}: {e}. Starting fresh.\")\n",
    "            last_processed_index = -1\n",
    "\n",
    "    questions_processed_since_resume = 0\n",
    "    total_to_process = len(df) - (last_processed_index + 1)\n",
    "    if total_to_process <= 0:\n",
    "        logger.info(\"No new questions to process based on existing output file.\")\n",
    "        return # Nothing left to do\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    for idx in tqdm(range(last_processed_index + 1, len(df)), desc=\"Processing Questions\", total=total_to_process, unit=\"q\"):\n",
    "        row = df.iloc[idx]\n",
    "        row_changed = False # Flag to check if we need to save\n",
    "\n",
    "        try:\n",
    "            # Process original question if its answer is missing\n",
    "            if pd.isna(df.at[idx, 'original_answer']):\n",
    "                original_question = str(row['original_question']).strip() if pd.notna(row['original_question']) else None\n",
    "                if original_question:\n",
    "                    answer = answerer.answer_question(original_question)\n",
    "                    df.at[idx, 'original_answer'] = answer\n",
    "                    row_changed = True\n",
    "                elif pd.notna(row['original_question']): # Handle case where question exists but was empty string\n",
    "                    df.at[idx, 'original_answer'] = \"Error: Missing/empty original question\"\n",
    "                    row_changed = True\n",
    "                # If original_question was NaN, leave original_answer as NaN\n",
    "\n",
    "            # Process generated question (if exists and its answer is missing)\n",
    "            if 'generated_question' in df.columns and 'faq_answer' in df.columns and pd.isna(df.at[idx, 'faq_answer']):\n",
    "                 generated_question = str(row['generated_question']).strip() if pd.notna(row['generated_question']) else None\n",
    "                 if generated_question:\n",
    "                      faq_answer = answerer.answer_question(generated_question)\n",
    "                      df.at[idx, 'faq_answer'] = faq_answer\n",
    "                      row_changed = True\n",
    "                 elif pd.notna(row['generated_question']): # Handle case where generated_question exists but was empty string\n",
    "                      df.at[idx, 'faq_answer'] = \"Error: Missing/empty generated question\"\n",
    "                      row_changed = True\n",
    "                 # If generated_question was NaN, leave faq_answer as NaN\n",
    "\n",
    "            if row_changed:\n",
    "                questions_processed_since_resume += 1\n",
    "\n",
    "                # Save progress periodically or at the very end\n",
    "                if (questions_processed_since_resume % SAVE_INTERVAL_INFERENCE == 0) or (idx == len(df) - 1):\n",
    "                     logger.info(f\"\\nSaving progress at index {idx}...\")\n",
    "                     try:\n",
    "                          df.to_csv(output_csv, index=False)\n",
    "                     except Exception as save_e:\n",
    "                          logger.error(f\"Failed to save progress to {output_csv}: {save_e}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "             logger.warning(\"\\nKeyboardInterrupt detected. Saving progress and exiting.\")\n",
    "             try:\n",
    "                 df.to_csv(output_csv, index=False)\n",
    "             except Exception as save_e:\n",
    "                 logger.error(f\"Failed to save progress during KeyboardInterrupt exit: {save_e}\")\n",
    "             raise # Re-raise interrupt\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error processing index {idx}: {e}. Recording error and saving progress.\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Record error in the specific row that failed, if possible\n",
    "            if pd.isna(df.at[idx, 'original_answer']): df.at[idx, 'original_answer'] = f\"Error: Processing Failed - {e}\"\n",
    "            if 'faq_answer' in df.columns and pd.isna(df.at[idx, 'faq_answer']): df.at[idx, 'faq_answer'] = f\"Error: Processing Failed - {e}\"\n",
    "            # Try to save the state including the error message\n",
    "            try:\n",
    "                df.to_csv(output_csv, index=False)\n",
    "            except Exception as save_e:\n",
    "                logger.error(f\"Failed to save error state to {output_csv}: {save_e}\")\n",
    "            continue # Attempt to continue with the next row\n",
    "\n",
    "    # --- Final Save ---\n",
    "    logger.info(\"Saving final results...\")\n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    except Exception as save_e:\n",
    "        logger.error(f\"Failed to save final results to {output_csv}: {save_e}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed processing {total_to_process} questions. Total time: {total_time / 60:.1f} minutes.\")\n",
    "    logger.info(f\"Generated answers saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    logger.info(\"--- Starting Medical FAQ Fine-Tuning and Processing Script ---\")\n",
    "\n",
    "    # --- 1. Load Base Model and Tokenizer ---\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    model_for_inference = None # This will hold the model to be used for answering\n",
    "\n",
    "    try:\n",
    "        model, tokenizer = load_base_model_and_tokenizer(BASE_MODEL_NAME)\n",
    "        # Initially, the model for inference is the base model\n",
    "        model_for_inference = model\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to load base model '{BASE_MODEL_NAME}'. Cannot proceed with fine-tuning or inference.\")\n",
    "        # load_base_model_and_tokenizer already logs traceback\n",
    "        return # Exit script if base model fails\n",
    "\n",
    "    # --- 2. Prepare Dataset for Fine-Tuning ---\n",
    "    dataset = process_dataset_for_fine_tuning(INPUT_CSV)\n",
    "\n",
    "    model_for_training = None # Initialize to None\n",
    "    if dataset is None:\n",
    "        logger.warning(\"Dataset preparation failed or resulted in no data. Skipping fine-tuning.\")\n",
    "    else:\n",
    "        logger.info(\"Dataset prepared successfully.\")\n",
    "\n",
    "        # --- 3. Apply PEFT Adapters (only if dataset is valid) ---\n",
    "        try:\n",
    "            # We apply PEFT adapters to the 'model' object loaded earlier\n",
    "            peft_model = apply_peft_to_model(model, tokenizer)\n",
    "            model_for_training = peft_model # This PEFT model will be trained\n",
    "            # Update the inference model to use the PEFT version IF PEFT application succeeds\n",
    "            model_for_inference = peft_model\n",
    "            logger.info(\"PEFT adapters applied successfully. Will use PEFT model for training and potentially inference.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to apply PEFT adapters: {e}. Skipping fine-tuning.\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            model_for_training = None # Ensure this is None if PEFT fails\n",
    "            # If PEFT fails, model_for_inference remains the original base 'model'\n",
    "\n",
    "    # --- 4. Fine-tune Model (only if dataset and PEFT model are ready) ---\n",
    "    # Check both dataset and model_for_training validity\n",
    "    if dataset is not None and model_for_training is not None:\n",
    "        logger.info(\"Proceeding with fine-tuning...\")\n",
    "        trained_model, tokenizer = fine_tune_model(model_for_training, tokenizer, dataset, FINE_TUNED_MODEL_DIR)\n",
    "\n",
    "        if trained_model is not None:\n",
    "             logger.info(\"Fine-tuning process completed (or attempted). Using the resulting model for inference.\")\n",
    "             # Update the inference model to the one returned by fine_tune_model\n",
    "             # This could be the model with trained adapters or the state before a training crash\n",
    "             model_for_inference = trained_model\n",
    "             # Optional: cleanup the reference used just for training if different\n",
    "             if model_for_training is not trained_model:\n",
    "                 del model_for_training\n",
    "             clear_gpu_memory()\n",
    "        else:\n",
    "             logger.warning(\"Fine-tuning function returned None (likely due to critical error). Inference will use the model state from *before* the fine_tune_model call.\")\n",
    "             # model_for_inference is already set to peft_model (if PEFT succeeded) or base model (if PEFT failed)\n",
    "    elif dataset is None:\n",
    "         logger.warning(\"Skipping fine-tuning because dataset preparation failed.\")\n",
    "         # model_for_inference remains the base model 'model' or peft_model if PEFT applied but dataset failed later\n",
    "    else: # model_for_training must be None because PEFT failed\n",
    "         logger.warning(\"Skipping fine-tuning because PEFT adapter application failed.\")\n",
    "         # model_for_inference remains the base model 'model'\n",
    "\n",
    "\n",
    "    # --- 5. Inference Phase ---\n",
    "    logger.info(\"--- Starting Inference Phase ---\")\n",
    "    if model_for_inference is None:\n",
    "        # This case should ideally not be reached if base model loading succeeded\n",
    "        logger.error(\"No valid model available for inference (should have at least the base model). Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the final model for inference is in evaluation mode\n",
    "    model_for_inference.eval()\n",
    "\n",
    "    # Log which model configuration is being used for inference\n",
    "    logger.info(f\"Preparing for inference using model type: {type(model_for_inference)}\")\n",
    "    if isinstance(model_for_inference, PeftModel):\n",
    "         logger.info(\"Inference will use the PEFT model (either freshly adapted or fine-tuned).\")\n",
    "    else:\n",
    "         logger.info(\"Inference will use the original BASE model (fine-tuning was skipped or failed).\")\n",
    "\n",
    "    # Run the inference process\n",
    "    clear_gpu_memory()\n",
    "    process_questions_file(INPUT_CSV, OUTPUT_CSV, model_for_inference, tokenizer)\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Basic check for psutil, often needed for memory monitoring/limits\n",
    "    try:\n",
    "        import psutil\n",
    "    except ImportError:\n",
    "        print(\"Error: psutil library not found. Please install it: pip install psutil\")\n",
    "        # Optionally exit, or let the script fail later if psutil is strictly needed\n",
    "        # exit()\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:CUDA not available. Using device: CPU\n",
      "INFO:__main__:--- Starting Medical FAQ Fine-Tuning and Processing Script ---\n",
      "INFO:__main__:Attempting to load base model: malhajar/meditron-7b-chat\n",
      "INFO:__main__:Tokenizer loaded successfully.\n",
      "INFO:__main__:Setting chat template for Meditron model.\n",
      "WARNING:__main__:Tokenizer does not have a pad token. Setting to eos_token.\n",
      "CRITICAL:__main__:Failed to load base model 'malhajar/meditron-7b-chat'. Cannot proceed with fine-tuning or inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union\n",
    "import psutil\n",
    "import traceback\n",
    "\n",
    "# Try importing necessary libraries early\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        BitsAndBytesConfig,\n",
    "        DataCollatorForSeq2Seq\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "    import accelerate\n",
    "    import bitsandbytes\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}\")\n",
    "    print(\"Please ensure transformers, peft, datasets, accelerate, bitsandbytes, and psutil are installed.\")\n",
    "    exit()\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Control GPU visibility if needed\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    logger.info(f\"CUDA available. Using device: {device}\")\n",
    "    try:\n",
    "        gpu_index = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "        logger.info(f\"CUDA device name: {gpu_name}\")\n",
    "        t = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        r = torch.cuda.memory_reserved(gpu_index)\n",
    "        a = torch.cuda.memory_allocated(gpu_index)\n",
    "        f = r - a\n",
    "        logger.info(f\"Initial GPU Memory (Bytes): Total={t}, Reserved={r}, Allocated={a}\")\n",
    "        logger.info(f\"Initial GPU Memory (GB): Total={t/1e9:.2f}GB, Reserved={r/1e9:.2f}GB, Allocated={a/1e9:.2f}GB, FreeReserved={f/1e9:.2f}GB\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not get GPU details: {e}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    logger.info(\"CUDA not available. Using device: CPU\")\n",
    "\n",
    "# Global configuration\n",
    "INPUT_CSV = \"T5_FAQS1.csv\"\n",
    "OUTPUT_CSV = \"medical_answers_finetuned_v7.csv\" # Incremented version\n",
    "FINE_TUNED_MODEL_DIR = \"fine_tuned_medical_model_v6\" # Incremented version\n",
    "BASE_MODEL_NAME = \"malhajar/meditron-7b-chat\"\n",
    "MAX_DATASET_EXAMPLES = 50 # Limit examples for faster testing/demo\n",
    "MAX_TRAINING_STEPS = 50   # Limit training steps for faster testing/demo\n",
    "MAX_SEQ_LENGTH_COLLATOR = 128 # Max length for sequences during training (affects memory)\n",
    "MAX_SEQ_LENGTH_INFERENCE = 500 # Max length for input sequence during inference\n",
    "INFERENCE_BATCH_SIZE = 1 # Process one question at a time for inference\n",
    "SAVE_INTERVAL_INFERENCE = 5 # Save progress every N questions during inference\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clears GPU memory.\"\"\"\n",
    "    logger.debug(\"Clearing GPU Cache...\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        logger.debug(\"GPU Cache Cleared.\")\n",
    "    else:\n",
    "        logger.debug(\"No GPU available, skipping cache clearing.\")\n",
    "\n",
    "def load_base_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads the base quantized model and tokenizer with explicit memory limits.\"\"\"\n",
    "    logger.info(f\"Attempting to load base model: {model_name}\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 1. Load Tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        logger.info(\"Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load tokenizer for {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2. Set Chat Template & Padding Token\n",
    "    if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
    "        logger.info(\"Setting chat template for Meditron model.\")\n",
    "        # Basic template structure - adjust if needed based on model card\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}### Response:\n",
    "{{ message['content'] }}\n",
    "{% endif %}{% if loop.last and add_generation_prompt %}### Response:\n",
    "{% endif %}{% endfor %}\"\"\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        logger.warning(\"Tokenizer does not have a pad token. Setting to eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id # Explicitly set ID\n",
    "\n",
    "    # 3. Configure Quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    logger.info(f\"Using quantization config: {quantization_config}\")\n",
    "\n",
    "    # 4. Define Memory Limits (Using INTEGER key for GPU)\n",
    "    max_memory = {}\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_index = 0 # Assuming device 0\n",
    "        total_vram_bytes = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        # Leave ~2GB buffer for safety, adjust if needed\n",
    "        gpu_mem_limit_bytes = total_vram_bytes - int(2 * 1024**3)\n",
    "        max_memory[gpu_index] = f\"{gpu_mem_limit_bytes // (1024**2)}MiB\" # Use integer index\n",
    "        logger.info(f\"Calculated GPU memory limit for device {gpu_index}: {max_memory[gpu_index]}\")\n",
    "    total_ram_bytes = psutil.virtual_memory().total\n",
    "    # Limit CPU RAM usage to 80% to avoid system freeze\n",
    "    cpu_mem_limit_bytes = int(total_ram_bytes * 0.80)\n",
    "    max_memory['cpu'] = f\"{cpu_mem_limit_bytes // (1024**2)}MiB\"\n",
    "\n",
    "    logger.info(f\"Setting max_memory for accelerate: {max_memory}\")\n",
    "\n",
    "    # 5. Load Model\n",
    "    try:\n",
    "        logger.info(\"Loading 4-bit quantized model with device_map='auto' and max_memory...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\", # Automatically distribute model layers across devices\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.float16, # Use float16 for memory efficiency\n",
    "            low_cpu_mem_usage=True, # Try to load shards sequentially to save CPU RAM\n",
    "            max_memory=max_memory, # Apply memory limits per device\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        logger.info(\"Base model loaded successfully.\")\n",
    "        logger.info(f\"Model device map: {model.hf_device_map}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load base model {model_name} even with max_memory: {e}\")\n",
    "        logger.error(traceback.format_exc()) # Log full traceback\n",
    "        raise\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_peft_to_model(model, tokenizer):\n",
    "    \"\"\"Applies PEFT/LoRA adapters to the loaded base model.\"\"\"\n",
    "    logger.info(\"Applying PEFT/LoRA adapters to the model...\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 1. Prepare model for k-bit training\n",
    "    logger.info(\"Preparing model for k-bit training...\")\n",
    "    # Ensure gradient checkpointing is enabled here if desired for training\n",
    "    try:\n",
    "         # use_gradient_checkpointing=True can sometimes cause issues, let TrainingArguments handle it primarily.\n",
    "         # Set it to False here, rely on TrainingArguments.gradient_checkpointing=True\n",
    "         model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "         logger.info(\"prepare_model_for_kbit_training successful.\")\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Error during prepare_model_for_kbit_training: {e}\")\n",
    "         raise\n",
    "\n",
    "    # 2. Ensure input embeddings require gradients (can be crucial for some models/setups)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        logger.info(\"Enabling input require grads using model.enable_input_require_grads().\")\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        # Fallback method if the direct function isn't available\n",
    "        logger.info(\"Attempting to enable input require grads using forward hook.\")\n",
    "        try:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                 if isinstance(output, torch.Tensor) and output.is_floating_point():\n",
    "                     output.requires_grad_(True)\n",
    "            embed_module = model.get_input_embeddings()\n",
    "            if embed_module:\n",
    "                 embed_module.register_forward_hook(make_inputs_require_grad)\n",
    "                 logger.info(\"Gradient hook attached to input embeddings.\")\n",
    "            else:\n",
    "                 logger.warning(\"Could not find input embeddings module.\")\n",
    "        except Exception as e:\n",
    "             logger.warning(f\"Failed to attach gradient hook: {e}. This might be okay.\")\n",
    "\n",
    "    # 3. Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Rank of the update matrices (lower value = fewer parameters)\n",
    "        lora_alpha=16, # LoRA scaling factor\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Modules to apply LoRA to (check model architecture if needed)\n",
    "        lora_dropout=0.05, # Dropout probability for LoRA layers\n",
    "        bias=\"none\", # Whether to train biases ('none', 'all', or 'lora_only')\n",
    "        task_type=\"CAUSAL_LM\" # Task type for PEFT\n",
    "    )\n",
    "    logger.info(f\"Using LoRA config: {lora_config}\")\n",
    "\n",
    "    # 4. Apply LoRA using get_peft_model\n",
    "    try:\n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        logger.info(\"PEFT model created successfully using get_peft_model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to apply PEFT to the model using get_peft_model: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 5. Verify trainable parameters\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # 6. <<< ADDED STEP: Explicitly mark as PEFT model >>>\n",
    "    # This might help older/buggy Trainer checks, although usually not needed.\n",
    "    if not isinstance(peft_model, PeftModel):\n",
    "         logger.warning(\"get_peft_model did not return a PeftModel instance!\")\n",
    "    else:\n",
    "         logger.info(\"Model is instance of PeftModel. Setting is_peft_model=True attribute just in case.\")\n",
    "         # Use setattr for safety in case attribute doesn't exist on all versions\n",
    "         setattr(peft_model, 'is_peft_model', True)\n",
    "\n",
    "    logger.info(f\"Model type after get_peft_model: {type(peft_model)}\")\n",
    "    return peft_model\n",
    "\n",
    "def process_dataset_for_fine_tuning(csv_file):\n",
    "    \"\"\"Loads and formats the dataset for fine-tuning, focusing on ideal_answer format.\"\"\"\n",
    "    logger.info(f\"Processing dataset from {csv_file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        logger.info(f\"Loaded dataframe with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Dataset file not found: {csv_file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['original_question', 'ideal_answer']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        logger.error(f\"Dataset missing required columns: {required_cols}. Found: {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Drop rows where essential columns are missing\n",
    "    df.dropna(subset=required_cols, inplace=True)\n",
    "    logger.info(f\"Rows after dropping NA in required columns: {len(df)}\")\n",
    "\n",
    "    if df.empty:\n",
    "        logger.error(\"No valid data remaining after filtering.\")\n",
    "        return None\n",
    "\n",
    "    # Limit dataset size if configured\n",
    "    if len(df) > MAX_DATASET_EXAMPLES:\n",
    "        logger.info(f\"Limiting dataset to {MAX_DATASET_EXAMPLES} examples (from {len(df)})\")\n",
    "        df = df.sample(n=MAX_DATASET_EXAMPLES, random_state=42) # Use sampling with fixed state\n",
    "\n",
    "    logger.info(f\"Using {len(df)} examples for fine-tuning.\")\n",
    "\n",
    "    # Define the system prompt - updated to match BioASQ ideal answer format\n",
    "    system_prompt = \"You are a biomedical expert. Provide accurate, concise, and comprehensive answers to medical questions. Your answers should be one-paragraph summaries that address the question completely, based on current medical knowledge.\"\n",
    "\n",
    "    # Format data into conversation structure\n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = str(row['original_question']).strip()\n",
    "        answer = str(row['ideal_answer']).strip()\n",
    "        if not question or not answer: # Skip rows with empty question or answer\n",
    "            continue\n",
    "        # Structure expected by tokenizer.apply_chat_template\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer} # The expected completion\n",
    "        ]\n",
    "        formatted_data.append({\"conversation\": conversation})\n",
    "\n",
    "    if not formatted_data:\n",
    "         logger.error(\"No valid conversation pairs found after formatting.\")\n",
    "         return None\n",
    "\n",
    "    # Create Hugging Face Dataset object\n",
    "    try:\n",
    "        dataset = Dataset.from_list(formatted_data)\n",
    "        logger.info(\"HuggingFace Dataset created successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create HuggingFace Dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class MedicalChatDataCollator:\n",
    "    \"\"\"Formats conversations and prepares inputs/labels for Causal LM fine-tuning.\"\"\"\n",
    "    def __init__(self, tokenizer, max_length=MAX_SEQ_LENGTH_COLLATOR):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        if not hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "             raise ValueError(\"Tokenizer must have `apply_chat_template` method.\")\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            logger.warning(\"Collator: Tokenizer pad_token_id is None. Using eos_token_id.\")\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[Dict[str, str]]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch_conversations = [ex[\"conversation\"] for ex in examples]\n",
    "\n",
    "        try:\n",
    "             # Prepare Inputs (prompt part only: system + user)\n",
    "             # We add `add_generation_prompt=True` which typically adds the assistant prompt start (e.g., \"### Response:\")\n",
    "             input_formatted = [\n",
    "                  self.tokenizer.apply_chat_template(conv[:-1], tokenize=False, add_generation_prompt=True)\n",
    "                  for conv in batch_conversations\n",
    "             ]\n",
    "             # Tokenize the inputs\n",
    "             model_inputs = self.tokenizer(\n",
    "                  input_formatted,\n",
    "                  max_length=self.max_length,\n",
    "                  padding=\"max_length\", # Pad to max_length\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\"\n",
    "             )\n",
    "\n",
    "             # Prepare Labels (full conversation: system + user + assistant)\n",
    "             # We do NOT add generation prompt here, as we want the full sequence including the assistant's answer.\n",
    "             labels_formatted = [\n",
    "                  self.tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n",
    "                  for conv in batch_conversations\n",
    "             ]\n",
    "             # Tokenize the full conversation to get the target labels\n",
    "             labels = self.tokenizer(\n",
    "                  labels_formatted,\n",
    "                  max_length=self.max_length,\n",
    "                  padding=\"max_length\", # Pad to max_length\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\"\n",
    "             )[\"input_ids\"]\n",
    "\n",
    "             # --- Label Masking ---\n",
    "             # We only want to compute loss on the assistant's response tokens.\n",
    "             # Mask tokens belonging to the system prompt, user query, and padding.\n",
    "             masked_labels = labels.clone()\n",
    "\n",
    "             for i in range(len(model_inputs[\"input_ids\"])):\n",
    "                  # Calculate the length of the input prompt (system + user + assistant prompt start)\n",
    "                  # Use attention_mask sum, as input_ids might contain padding AFTER truncation\n",
    "                  input_ids_len = model_inputs[\"attention_mask\"][i].sum().item()\n",
    "\n",
    "                  # Mask all tokens up to the end of the input prompt\n",
    "                  masked_labels[i, :input_ids_len] = -100 # -100 is the standard ignore index for loss calculation\n",
    "\n",
    "                  # Also mask padding tokens in the labels\n",
    "                  # Find indices where labels are the padding token ID\n",
    "                  label_pad_indices = (labels[i] == self.tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "                  if len(label_pad_indices) > 0:\n",
    "                      # Mask all padding tokens (usually at the end)\n",
    "                      # Get the index of the first padding token\n",
    "                      first_pad_index = label_pad_indices[0].item()\n",
    "                      masked_labels[i, first_pad_index:] = -100\n",
    "\n",
    "             # Check if any example ended up with all labels masked (problematic)\n",
    "             if torch.all(masked_labels == -100, dim=1).any():\n",
    "                  logger.warning(\"Warning: An example has all labels masked. This might indicate issues with sequence lengths, truncation, or the collator logic.\")\n",
    "                  # Consider adding more detailed logging here if this happens frequently\n",
    "\n",
    "             model_inputs[\"labels\"] = masked_labels\n",
    "             return model_inputs\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error in Data Collator: {e}\")\n",
    "             logger.error(traceback.format_exc())\n",
    "             # Return empty batch or raise error? Returning empty might cause issues later.\n",
    "             raise # Let the error propagate\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, output_dir):\n",
    "    \"\"\"Fine-tunes the PEFT model using the Trainer API.\"\"\"\n",
    "    logger.info(\"Starting fine-tuning process...\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1, # Single epoch for quick demo/test\n",
    "        per_device_train_batch_size=1, # Small batch size due to model size/memory\n",
    "        gradient_accumulation_steps=16, # Accumulate gradients to simulate larger batch size (effective batch size = 1 * 16 = 16)\n",
    "        learning_rate=2e-4, # Common learning rate for LoRA\n",
    "        weight_decay=0.01, # Regularization\n",
    "        warmup_ratio=0.03, # Warmup steps as a fraction of total steps\n",
    "        logging_steps=5, # Log metrics every 5 steps\n",
    "        save_strategy=\"steps\", # Save checkpoints based on steps\n",
    "        save_steps=max(1, MAX_TRAINING_STEPS // 2), # Save halfway through (adjust as needed)\n",
    "        save_total_limit=1, # Keep only the latest checkpoint\n",
    "        fp16=torch.cuda.is_available(), # Enable mixed-precision training if CUDA is available\n",
    "        gradient_checkpointing=True, # Use gradient checkpointing to save memory (at cost of compute) - RELY ON THIS\n",
    "        optim=\"paged_adamw_8bit\", # Use paged AdamW optimizer for memory efficiency with QLoRA\n",
    "        max_grad_norm=0.3, # Gradient clipping\n",
    "        dataloader_num_workers=0, # Set to 0 or small number, can cause issues otherwise\n",
    "        dataloader_pin_memory=False, # Often False is better with device_map='auto'\n",
    "        max_steps=MAX_TRAINING_STEPS, # Limit total training steps\n",
    "        report_to=\"none\", # Disable external reporting (like wandb) for simplicity\n",
    "        push_to_hub=False, # Don't push to Hugging Face Hub\n",
    "        remove_unused_columns=True, # Let Trainer remove columns not used by the model\n",
    "        # Added for potential stability\n",
    "        # ddp_find_unused_parameters=False # Sometimes needed with PEFT/gradient checkpointing\n",
    "    )\n",
    "    logger.info(f\"Using Training Arguments: {training_args}\")\n",
    "\n",
    "    # Data Collator Instance\n",
    "    data_collator = MedicalChatDataCollator(tokenizer, max_length=MAX_SEQ_LENGTH_COLLATOR)\n",
    "\n",
    "    # === Debugging Logs Before Trainer Init ===\n",
    "    logger.info(f\"--- Preparing to initialize Trainer ---\")\n",
    "    logger.info(f\"Model object ID: {id(model)}\")\n",
    "    logger.info(f\"Model type passed to fine_tune_model: {type(model)}\")\n",
    "    logger.info(f\"Is model instance of PeftModel? {isinstance(model, PeftModel)}\")\n",
    "    logger.info(f\"Does model have 'peft_config' attribute? {hasattr(model, 'peft_config')}\")\n",
    "    logger.info(f\"Does model have 'is_peft_model' attribute set? {getattr(model, 'is_peft_model', 'Not Set')}\")\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "         logger.info(f\"Model device map: {model.hf_device_map}\")\n",
    "    else:\n",
    "         logger.info(\"Model does not have 'hf_device_map' attribute.\")\n",
    "    if hasattr(model, 'is_quantized'):\n",
    "         logger.info(f\"Model is_quantized: {model.is_quantized}\")\n",
    "    else:\n",
    "         logger.info(\"Model does not have 'is_quantized' attribute.\")\n",
    "    # Check base model properties if it's a PeftModel\n",
    "    if isinstance(model, PeftModel) and hasattr(model, 'base_model'):\n",
    "         base = model.base_model\n",
    "         logger.info(f\"Base model type: {type(base)}\")\n",
    "         logger.info(f\"Base model is_quantized: {getattr(base, 'is_quantized', 'Not Set')}\")\n",
    "         logger.info(f\"Base model has quantization_config: {hasattr(base.config, 'quantization_config')}\")\n",
    "         if hasattr(base.config, 'quantization_config'):\n",
    "              logger.info(f\"Base model quantization_config type: {type(base.config.quantization_config)}\")\n",
    "    logger.info(f\"--- End Pre-Trainer Init Logs ---\")\n",
    "    # === End Debugging Logs ===\n",
    "\n",
    "    # Trainer Initialization\n",
    "    trainer = None # Initialize to None\n",
    "    try:\n",
    "        trainer = Trainer(\n",
    "            model=model, # Should be the PeftModel instance from apply_peft_to_model\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            # callbacks=[...] # Add custom callbacks if needed\n",
    "        )\n",
    "        logger.info(\"Trainer initialized successfully.\")\n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"ValueError during Trainer initialization: {ve}\")\n",
    "        logger.error(\"This likely means the Trainer still doesn't recognize the model as PEFT-compatible, possibly due to issues with quantization or PEFT setup.\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, tokenizer # Cannot proceed if Trainer fails\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during Trainer initialization: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, tokenizer\n",
    "\n",
    "    # Training\n",
    "    logger.info(\"Starting training...\")\n",
    "    trained_model = None\n",
    "    try:\n",
    "        # Start training\n",
    "        train_result = trainer.train()\n",
    "        logger.info(\"Training completed.\")\n",
    "        # Log metrics\n",
    "        metrics = train_result.metrics\n",
    "        logger.info(f\"Training metrics: {metrics}\")\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trained_model = model # Assign model if training successful\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        # Attempt to save state even if training failed mid-way\n",
    "        try:\n",
    "            logger.warning(\"Attempting to save model state after training error...\")\n",
    "            save_path = f\"{output_dir}/error_save\"\n",
    "            if trainer is not None:\n",
    "                 trainer.save_model(output_dir=save_path)\n",
    "                 tokenizer.save_pretrained(save_path)\n",
    "                 logger.info(f\"Model state saved to {save_path}\")\n",
    "            else:\n",
    "                 logger.error(\"Trainer was not initialized, cannot save model.\")\n",
    "        except Exception as save_e:\n",
    "            logger.error(f\"Could not save model after error: {save_e}\")\n",
    "        # Return None for model if training failed critically\n",
    "        return None, tokenizer\n",
    "\n",
    "    # Save final model adapters (LoRA weights) and tokenizer\n",
    "    if trained_model is not None:\n",
    "        logger.info(f\"Saving fine-tuned PEFT adapters and tokenizer to {output_dir}\")\n",
    "        try:\n",
    "            # save_model() with PEFT model saves only the adapters by default\n",
    "            trainer.save_model(output_dir)\n",
    "            # Save the tokenizer configuration as well\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            logger.info(\"Model adapters and tokenizer saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving final model/tokenizer: {e}\")\n",
    "            pass # Return the model object anyway, even if saving failed\n",
    "    else:\n",
    "         logger.warning(\"Training did not complete successfully, final model not saved via trainer.save_model.\")\n",
    "\n",
    "    # Cleanup\n",
    "    del trainer\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return trained_model, tokenizer # Return the model (potentially with trained adapters)\n",
    "\n",
    "class MedicalQuestionAnswerer:\n",
    "    \"\"\"Generates answers using the fine-tuned PEFT model, formatted as ideal answers.\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        try:\n",
    "             # If using device_map, the device might be complex, find a parameter's device\n",
    "             self.device = next(model.parameters()).device\n",
    "        except Exception:\n",
    "             logger.warning(\"Could not automatically determine model device. Assuming CPU or first CUDA device if available.\")\n",
    "             self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        \n",
    "        # Updated system prompt to match BioASQ ideal answer format\n",
    "        self.sys_message = \"You are a biomedical expert. Provide a concise, one-paragraph answer that fully addresses the medical question. Your answer should be factual, accurate, and based on current medical knowledge, written for other experts in the field.\"\n",
    "        \n",
    "        logger.info(f\"Question Answerer initialized on device: {self.device}\")\n",
    "\n",
    "        # Ensure pad token is set for generation\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    @torch.inference_mode() # Disable gradient calculations for inference\n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Generates an ideal answer for a medical question.\"\"\"\n",
    "        if not question or not isinstance(question, str):\n",
    "             logger.warning(f\"Invalid question provided: '{question}'. Skipping.\")\n",
    "             return \"Error: Invalid question provided.\"\n",
    "\n",
    "        logger.debug(f\"Answering question: '{question[:50]}...'\")\n",
    "        clear_gpu_memory() # Clear cache before generating\n",
    "\n",
    "        try:\n",
    "            # Prepare input using the chat template\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.sys_message},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True, # Add the prompt for the assistant's turn\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device) # Move inputs to the model's device\n",
    "\n",
    "            input_length = inputs.shape[1]\n",
    "\n",
    "            # Optional: Check if input exceeds inference length limit\n",
    "            if input_length >= MAX_SEQ_LENGTH_INFERENCE:\n",
    "                 logger.warning(f\"Input sequence length ({input_length}) is >= MAX_SEQ_LENGTH_INFERENCE ({MAX_SEQ_LENGTH_INFERENCE}). Input might be truncated by model implicitly or cause issues.\")\n",
    "\n",
    "            # --- Generate the answer ---\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs,\n",
    "                # Parameters adjusted for BioASQ ideal answer format\n",
    "                max_new_tokens=450, # Ideal answers are typically one paragraph\n",
    "                temperature=0.5,    # Lower temperature for more factual responses\n",
    "                top_p=0.9,          # Nucleus sampling parameter\n",
    "                do_sample=True,     # Enable sampling strategies\n",
    "                num_beams=3,        # Use beam search for better coherence\n",
    "                no_repeat_ngram_size=3, # Avoid repetition of phrases\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True      # Speed up generation\n",
    "            )\n",
    "\n",
    "            # Decode only the newly generated tokens\n",
    "            output_tokens = outputs[0, input_length:]\n",
    "            answer = self.tokenizer.decode(output_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Format the answer as a single paragraph (BioASQ ideal answer format)\n",
    "            answer = ' '.join(answer.split())\n",
    "\n",
    "            logger.debug(f\"Generated ideal answer: '{answer[:100]}...'\")\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer for question '{question[:50]}...': {str(e)}\")\n",
    "            logger.debug(traceback.format_exc()) # Log traceback for debugging\n",
    "            return f\"Error: Processing failed during generation.\"\n",
    "        finally:\n",
    "            clear_gpu_memory() # Clear cache after generating\n",
    "\n",
    "def process_questions_file(input_csv, output_csv, model, tokenizer):\n",
    "    \"\"\"Processes questions from CSV sequentially with resuming and saving.\"\"\"\n",
    "    logger.info(f\"Starting sequential question processing from {input_csv}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"Loaded {len(df)} questions from {input_csv}.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input CSV not found: {input_csv}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading input CSV {input_csv}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize the question answerer class\n",
    "    answerer = MedicalQuestionAnswerer(model, tokenizer)\n",
    "\n",
    "    # Ensure output columns exist, initialize with NA if not\n",
    "    if 'original_answer' not in df.columns: df['original_answer'] = pd.NA\n",
    "    # Check if 'ideal_answer' column exists, if not create it\n",
    "    if 'ideal_answer' not in df.columns: df['ideal_answer'] = pd.NA\n",
    "    # Check if 'faq_answer' should exist based on input columns\n",
    "    if 'generated_question' in df.columns and 'faq_answer' not in df.columns:\n",
    "        df['faq_answer'] = pd.NA\n",
    "\n",
    "    # --- Resuming Logic ---\n",
    "    last_processed_index = -1\n",
    "    if os.path.exists(output_csv):\n",
    "        logger.info(f\"Output file {output_csv} found. Attempting to resume.\")\n",
    "        try:\n",
    "            df_existing = pd.read_csv(output_csv)\n",
    "            # Find the last row where 'ideal_answer' is NOT NA\n",
    "            valid_indices = df_existing['ideal_answer'].dropna().index\n",
    "            if not valid_indices.empty:\n",
    "                 last_processed_index = valid_indices[-1]\n",
    "                 logger.info(f\"Resuming from index {last_processed_index + 1}.\")\n",
    "                 # Update the current dataframe with already processed answers from the existing file\n",
    "                 # Only update up to the last processed index to avoid overwriting potentially newer data\n",
    "                 df.update(df_existing.iloc[:last_processed_index+1])\n",
    "            else:\n",
    "                logger.info(\"No previously processed answers found in output file, starting fresh.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "             logger.warning(f\"Output file {output_csv} is empty. Starting fresh.\")\n",
    "             last_processed_index = -1\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read or parse existing output file {output_csv}: {e}. Starting fresh.\")\n",
    "            last_processed_index = -1\n",
    "\n",
    "    questions_processed_since_resume = 0\n",
    "    total_to_process = len(df) - (last_processed_index + 1)\n",
    "    if total_to_process <= 0:\n",
    "        logger.info(\"No new questions to process based on existing output file.\")\n",
    "        return # Nothing left to do\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    for idx in tqdm(range(last_processed_index + 1, len(df)), desc=\"Processing Questions\", total=total_to_process, unit=\"q\"):\n",
    "        row = df.iloc[idx]\n",
    "        row_changed = False # Flag to check if we need to save\n",
    "\n",
    "        try:\n",
    "            # Process original question if its ideal answer is missing\n",
    "            if pd.isna(df.at[idx, 'ideal_answer']):\n",
    "                original_question = str(row['original_question']).strip() if pd.notna(row['original_question']) else None\n",
    "                if original_question:\n",
    "                    # Generate ideal answer format response\n",
    "                    ideal_answer = answerer.answer_question(original_question)\n",
    "                    df.at[idx, 'ideal_answer'] = ideal_answer\n",
    "                    row_changed = True\n",
    "                elif pd.notna(row['original_question']): # Handle case where question exists but was empty string\n",
    "                    df.at[idx, 'ideal_answer'] = \"Error: Missing/empty original question\"\n",
    "                    row_changed = True\n",
    "                # If original_question was NaN, leave ideal_answer as NaN\n",
    "\n",
    "            # Process original question if its answer is missing (for backward compatibility)\n",
    "            if pd.isna(df.at[idx, 'original_answer']):\n",
    "                original_question = str(row['original_question']).strip() if pd.notna(row['original_question']) else None\n",
    "                if original_question:\n",
    "                    # If we already generated an ideal_answer, use it for original_answer too\n",
    "                    if pd.notna(df.at[idx, 'ideal_answer']):\n",
    "                        df.at[idx, 'original_answer'] = df.at[idx, 'ideal_answer']\n",
    "                    else:\n",
    "                        answer = answerer.answer_question(original_question)\n",
    "                        df.at[idx, 'original_answer'] = answer\n",
    "                    row_changed = True\n",
    "                elif pd.notna(row['original_question']): # Handle case where question exists but was empty string\n",
    "                    df.at[idx, 'original_answer'] = \"Error: Missing/empty original question\"\n",
    "                    row_changed = True\n",
    "                # If original_question was NaN, leave original_answer as NaN\n",
    "\n",
    "            # Process generated question (if exists and its answer is missing)\n",
    "            if 'generated_question' in df.columns and 'faq_answer' in df.columns and pd.isna(df.at[idx, 'faq_answer']):\n",
    "                 generated_question = str(row['generated_question']).strip() if pd.notna(row['generated_question']) else None\n",
    "                 if generated_question:\n",
    "                      faq_answer = answerer.answer_question(generated_question)\n",
    "                      df.at[idx, 'faq_answer'] = faq_answer\n",
    "                      row_changed = True\n",
    "                 elif pd.notna(row['generated_question']): # Handle case where generated_question exists but was empty string\n",
    "                      df.at[idx, 'faq_answer'] = \"Error: Missing/empty generated question\"\n",
    "                      row_changed = True\n",
    "                 # If generated_question was NaN, leave faq_answer as NaN\n",
    "\n",
    "            if row_changed:\n",
    "                 questions_processed_since_resume += 1\n",
    "                 if (questions_processed_since_resume % SAVE_INTERVAL_INFERENCE == 0) or (idx == len(df) - 1):\n",
    "                     logger.info(f\"\\nSaving progress at index {idx}...\")\n",
    "                     try:\n",
    "                          df.to_csv(output_csv, index=False)\n",
    "                     except Exception as save_e:\n",
    "                          logger.error(f\"Failed to save progress to {output_csv}: {save_e}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "             logger.warning(\"\\nKeyboardInterrupt detected. Saving progress and exiting.\")\n",
    "             try:\n",
    "                 df.to_csv(output_csv, index=False)\n",
    "             except Exception as save_e:\n",
    "                 logger.error(f\"Failed to save progress during KeyboardInterrupt exit: {save_e}\")\n",
    "             raise # Re-raise interrupt\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error processing index {idx}: {e}. Recording error and saving progress.\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            # Record error in the specific row that failed, if possible\n",
    "            if pd.isna(df.at[idx, 'ideal_answer']): df.at[idx, 'ideal_answer'] = f\"Error: Processing Failed - {e}\"\n",
    "            if pd.isna(df.at[idx, 'original_answer']): df.at[idx, 'original_answer'] = f\"Error: Processing Failed - {e}\"\n",
    "            if 'faq_answer' in df.columns and pd.isna(df.at[idx, 'faq_answer']): df.at[idx, 'faq_answer'] = f\"Error: Processing Failed - {e}\"\n",
    "            # Try to save the state including the error message\n",
    "            try:\n",
    "                df.to_csv(output_csv, index=False)\n",
    "            except Exception as save_e:\n",
    "                logger.error(f\"Failed to save error state to {output_csv}: {save_e}\")\n",
    "            continue # Attempt to continue with the next row\n",
    "\n",
    "    # --- Final Save ---\n",
    "    logger.info(\"Saving final results...\")\n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    except Exception as save_e:\n",
    "        logger.error(f\"Failed to save final results to {output_csv}: {save_e}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed processing {total_to_process} questions. Total time: {total_time / 60:.1f} minutes.\")\n",
    "    logger.info(f\"Generated answers saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "def main():\n",
    "    logger.info(\"--- Starting Medical FAQ Fine-Tuning and Processing Script ---\")\n",
    "\n",
    "    # --- 1. Load Base Model and Tokenizer ---\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    model_for_inference = None # This will hold the model to be used for answering\n",
    "\n",
    "    try:\n",
    "        model, tokenizer = load_base_model_and_tokenizer(BASE_MODEL_NAME)\n",
    "        # Initially, the model for inference is the base model\n",
    "        model_for_inference = model\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to load base model '{BASE_MODEL_NAME}'. Cannot proceed with fine-tuning or inference.\")\n",
    "        # load_base_model_and_tokenizer already logs traceback\n",
    "        return # Exit script if base model fails\n",
    "\n",
    "    # --- 2. Prepare Dataset for Fine-Tuning ---\n",
    "    dataset = process_dataset_for_fine_tuning(INPUT_CSV)\n",
    "\n",
    "    model_for_training = None # Initialize to None\n",
    "    if dataset is None:\n",
    "        logger.warning(\"Dataset preparation failed or resulted in no data. Skipping fine-tuning.\")\n",
    "    else:\n",
    "        logger.info(\"Dataset prepared successfully.\")\n",
    "\n",
    "        # --- 3. Apply PEFT Adapters (only if dataset is valid) ---\n",
    "        try:\n",
    "            # We apply PEFT adapters to the 'model' object loaded earlier\n",
    "            peft_model = apply_peft_to_model(model, tokenizer)\n",
    "            model_for_training = peft_model # This PEFT model will be trained\n",
    "            # Update the inference model to use the PEFT version IF PEFT application succeeds\n",
    "            model_for_inference = peft_model\n",
    "            logger.info(\"PEFT adapters applied successfully. Will use PEFT model for training and potentially inference.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to apply PEFT adapters: {e}. Skipping fine-tuning.\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            model_for_training = None # Ensure this is None if PEFT fails\n",
    "            # If PEFT fails, model_for_inference remains the original base 'model'\n",
    "\n",
    "    # --- 4. Fine-tune Model (only if dataset and PEFT model are ready) ---\n",
    "    # Check both dataset and model_for_training validity\n",
    "    if dataset is not None and model_for_training is not None:\n",
    "        logger.info(\"Proceeding with fine-tuning...\")\n",
    "        trained_model, tokenizer = fine_tune_model(model_for_training, tokenizer, dataset, FINE_TUNED_MODEL_DIR)\n",
    "\n",
    "        if trained_model is not None:\n",
    "             logger.info(\"Fine-tuning process completed (or attempted). Using the resulting model for inference.\")\n",
    "             # Update the inference model to the one returned by fine_tune_model\n",
    "             # This could be the model with trained adapters or the state before a training crash\n",
    "             model_for_inference = trained_model\n",
    "             # Optional: cleanup the reference used just for training if different\n",
    "             if model_for_training is not trained_model:\n",
    "                 del model_for_training\n",
    "             clear_gpu_memory()\n",
    "        else:\n",
    "             logger.warning(\"Fine-tuning function returned None (likely due to critical error). Inference will use the model state from *before* the fine_tune_model call.\")\n",
    "             # model_for_inference is already set to peft_model (if PEFT succeeded) or base model (if PEFT failed)\n",
    "    elif dataset is None:\n",
    "         logger.warning(\"Skipping fine-tuning because dataset preparation failed.\")\n",
    "         # model_for_inference remains the base model 'model' or peft_model if PEFT applied but dataset failed later\n",
    "    else: # model_for_training must be None because PEFT failed\n",
    "         logger.warning(\"Skipping fine-tuning because PEFT adapter application failed.\")\n",
    "         # model_for_inference remains the base model 'model'\n",
    "\n",
    "\n",
    "    # --- 5. Inference Phase ---\n",
    "    logger.info(\"--- Starting Inference Phase ---\")\n",
    "    if model_for_inference is None:\n",
    "        # This case should ideally not be reached if base model loading succeeded\n",
    "        logger.error(\"No valid model available for inference (should have at least the base model). Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the final model for inference is in evaluation mode\n",
    "    model_for_inference.eval()\n",
    "\n",
    "    # Log which model configuration is being used for inference\n",
    "    logger.info(f\"Preparing for inference using model type: {type(model_for_inference)}\")\n",
    "    if isinstance(model_for_inference, PeftModel):\n",
    "         logger.info(\"Inference will use the PEFT model (either freshly adapted or fine-tuned).\")\n",
    "    else:\n",
    "         logger.info(\"Inference will use the original BASE model (fine-tuning was skipped or failed).\")\n",
    "\n",
    "    # Run the inference process\n",
    "    clear_gpu_memory()\n",
    "    process_questions_file(INPUT_CSV, OUTPUT_CSV, model_for_inference, tokenizer)\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Basic check for psutil, often needed for memory monitoring/limits\n",
    "    try:\n",
    "        import psutil\n",
    "    except ImportError:\n",
    "        print(\"Error: psutil library not found. Please install it: pip install psutil\")\n",
    "        # Optionally exit, or let the script fail later if psutil is strictly needed\n",
    "        # exit()\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes==0.42.0\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy in /home/vjti/.local/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /home/vjti/.local/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install bitsandbytes==0.42.0 --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Final*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:CPU threads configured: 20\n",
      "INFO:__main__:CUDA available: False\n",
      "INFO:__main__:Loading model: malhajar/meditron-7b-chat\n",
      "INFO:__main__:Setting chat template for Meditron\n",
      "INFO:__main__:Set pad_token to eos_token\n",
      "INFO:__main__:Loading model for CPU execution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff29dca171114186814ff5a8ede1fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model loaded successfully\n",
      "INFO:__main__:Processing dataset from T5_FAQS1.csv\n",
      "INFO:__main__:Limiting dataset to 50 examples for CPU efficiency (from 16407)\n",
      "INFO:__main__:Dataset has 50 valid training examples\n",
      "INFO:__main__:Applying PEFT to the model...\n",
      "INFO:__main__:Trainable parameters: 8388608\n",
      "INFO:__main__:All parameters: 6746943488\n",
      "INFO:__main__:Trainable%: 0.1243%\n",
      "/home/vjti/.local/lib/python3.10/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of  Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "INFO:__main__:Model device before training: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/12 : < :, Epoch 0.16/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 407\u001b[0m\n\u001b[1;32m    404\u001b[0m         clear_gpu_memory()\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 390\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset:\n\u001b[1;32m    389\u001b[0m     model \u001b[38;5;241m=\u001b[39m apply_peft_to_model(model)\n\u001b[0;32m--> 390\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tuned_model_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Clear memory after training\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     clear_gpu_memory()\n",
      "Cell \u001b[0;32mIn[16], line 239\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, dataset, output_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    237\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device before training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1756\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1755\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1756\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    584\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m         position_embeddings,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:336\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:272\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    271\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    274\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    275\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:712\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    715\u001b[0m     lora_A_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "# FORCE CPU USAGE - Set before any other imports\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"20\"\n",
    "\n",
    "# Configure PyTorch for CPU only\n",
    "torch.set_num_threads(20)\n",
    "torch.cuda.is_available = lambda: False  # Force PyTorch to think CUDA is not available\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Force CPU usage\n",
    "device = torch.device(\"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "logger.info(f\"CPU threads configured: {torch.get_num_threads()}\")\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "def load_medical_model(model_name=\"malhajar/meditron-7b-chat\"):\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True,\n",
    "        use_fast=True\n",
    "    )\n",
    "    if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
    "        logger.info(\"Setting chat template for Meditron\")\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'system' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}### Response:\n",
    "{{ message['content'] }}\n",
    "{% endif %}\n",
    "{% if loop.last and add_generation_prompt %}### Response:\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        logger.info(\"Set pad_token to eos_token\")\n",
    "    gc.collect()\n",
    "    logger.info(\"Loading model for CPU execution...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # Ensure model is on CPU\n",
    "    model = model.to(device)\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_peft_to_model(model):\n",
    "    logger.info(\"Applying PEFT to the model...\")\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\" \n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    # Ensure PEFT model is on CPU\n",
    "    peft_model = peft_model.to(device)\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    logger.info(f\"Trainable parameters: {trainable_params}\")\n",
    "    logger.info(f\"All parameters: {all_params}\")\n",
    "    logger.info(f\"Trainable%: {100 * trainable_params / all_params:.4f}%\")\n",
    "    return peft_model\n",
    "\n",
    "def process_dataset_for_fine_tuning(csv_file):\n",
    "    logger.info(f\"Processing dataset from {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        logger.info(\"Creating a small sample dataset for testing\")\n",
    "        test_data = {\n",
    "            'original_question': [\n",
    "                \"What are the symptoms of diabetes?\",\n",
    "                \"How is hypertension diagnosed?\",\n",
    "                \"What are common treatments for migraine?\"\n",
    "            ],\n",
    "            'ideal_answer': [\n",
    "                \"Common symptoms of diabetes include frequent urination, increased thirst, unexplained weight loss, extreme hunger, blurred vision, tingling in the extremities, and frequent infections.\",\n",
    "                \"Hypertension is diagnosed when blood pressure readings consistently show systolic pressure above 130 mmHg or diastolic pressure above 80 mmHg. Diagnosis typically requires multiple readings over time.\",\n",
    "                \"Common treatments for migraine include pain relievers, triptans, anti-nausea medications, preventive medications like beta blockers, and lifestyle changes such as stress management and regular sleep.\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(test_data)\n",
    "    if 'original_question' not in df.columns or 'ideal_answer' not in df.columns:\n",
    "        logger.error(\"Dataset missing required columns (original_question and ideal_answer)\")\n",
    "        return None\n",
    "    df = df.dropna(subset=['original_question', 'ideal_answer'])\n",
    "    # Reduce dataset size more aggressively for CPU training\n",
    "    if len(df) > 50:\n",
    "        logger.info(f\"Limiting dataset to 50 examples for CPU efficiency (from {len(df)})\")\n",
    "        df = df.sample(50, random_state=42)\n",
    "    logger.info(f\"Dataset has {len(df)} valid training examples\")\n",
    "    system_prompt = \"You are an AI Medical Assistant. Give accurate and helpful answers to medical questions.\"\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": row['original_question']},\n",
    "            {\"role\": \"assistant\", \"content\": row['ideal_answer']}\n",
    "        ]\n",
    "        example = {\"conversation\": conversation}\n",
    "        train_data.append(example)\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "    return dataset\n",
    "\n",
    "class MedicalDataCollator:\n",
    "    def __init__(self, tokenizer, max_length=128):  # Reduced max_length for CPU\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        conversations = [ex[\"conversation\"] for ex in examples]\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "            input_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    conv[:-1],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                for conv in conversations\n",
    "            ]\n",
    "            target_texts = [\n",
    "                self.tokenizer.apply_chat_template(\n",
    "                    conv,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "                for conv in conversations\n",
    "            ]\n",
    "        else:\n",
    "            input_texts, target_texts = [], []\n",
    "            for conv in conversations:\n",
    "                system = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"system\"), \"\")\n",
    "                user = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"user\"), \"\")\n",
    "                assistant = next((msg[\"content\"] for msg in conv if msg[\"role\"] == \"assistant\"), \"\")\n",
    "                input_text = f\"### Instruction:\\n{system}\\n### Instruction:\\n{user}\\n### Response:\"\n",
    "                target_text = f\"### Instruction:\\n{system}\\n### Instruction:\\n{user}\\n### Response:\\n{assistant}\"\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "        \n",
    "        model_inputs = self.tokenizer(\n",
    "            input_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            target_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        labels_with_ignore_index = labels.clone()\n",
    "        labels_with_ignore_index[labels == self.tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "        \n",
    "        # Ensure all tensors are on CPU\n",
    "        for key in model_inputs:\n",
    "            if isinstance(model_inputs[key], torch.Tensor):\n",
    "                model_inputs[key] = model_inputs[key].to(device)\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, output_dir):\n",
    "    data_collator = MedicalDataCollator(tokenizer)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,  # Reduced epochs for CPU\n",
    "        per_device_train_batch_size=1,  # Reduced batch size\n",
    "        gradient_accumulation_steps=8,  # Increased to maintain effective batch size\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"no\",\n",
    "        learning_rate=3e-5,  # Slightly reduced learning rate\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=5,\n",
    "        fp16=False,  # Keep disabled for CPU\n",
    "        bf16=False,  # Ensure disabled for CPU\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        use_cpu=True,  # Explicitly force CPU usage\n",
    "        no_cuda=True,  # Explicitly disable CUDA\n",
    "        dataloader_pin_memory=False,  # Disable pin memory for CPU\n",
    "        dataloader_num_workers=0,  # Single worker for CPU\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Ensure model stays on CPU before training\n",
    "    model = model.to(device)\n",
    "    logger.info(f\"Model device before training: {next(model.parameters()).device}\")\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "class MedicalQuestionAnswerer:\n",
    "    def __init__(self, model, tokenizer, max_new_tokens=128):  # Reduced max tokens\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        # Ensure model is on CPU\n",
    "        self.model = self.model.to(device)\n",
    "    \n",
    "    def answer_question(self, question, system_prompt=\"You are an AI Medical Assistant.\"):\n",
    "        try:\n",
    "            conversation = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "                prompt = self.tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "            else:\n",
    "                prompt = f\"### Instruction:\\n{system_prompt}\\n### Instruction:\\n{question}\\n### Response:\"\n",
    "            \n",
    "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            \n",
    "            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if prompt in response_text:\n",
    "                answer = response_text.split(prompt)[-1].strip()\n",
    "            else:\n",
    "                answer = response_text.strip()\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {str(e)}\")\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "def process_questions_file(input_csv, output_csv, model, tokenizer, batch_size=1):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"Loaded dataset with {len(df)} questions\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    answerer = MedicalQuestionAnswerer(model, tokenizer)\n",
    "    \n",
    "    if 'original_answer' not in df.columns:\n",
    "        df['original_answer'] = \"\"\n",
    "    if 'faq_answer' not in df.columns:\n",
    "        df['faq_answer'] = \"\"\n",
    "    \n",
    "    last_processed = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.notna(row['original_answer']) and row['original_answer'] != \"\":\n",
    "            last_processed = i\n",
    "    \n",
    "    if last_processed > 0:\n",
    "        logger.info(f\"Resuming from question {last_processed+1}\")\n",
    "    \n",
    "    save_interval = 5  # More frequent saves\n",
    "    for i in range(last_processed, len(df), batch_size):\n",
    "        batch_end = min(i + batch_size, len(df))\n",
    "        batch_df = df.iloc[i:batch_end].copy()\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                if not pd.notna(row['original_answer']) or row['original_answer'] == \"\":\n",
    "                    original_question = row['original_question']\n",
    "                    answer = answerer.answer_question(original_question)\n",
    "                    df.at[idx, 'original_answer'] = answer\n",
    "                    logger.info(f\"Processed question {idx}\")\n",
    "                \n",
    "                if 'generated_question' in row and pd.notna(row['generated_question']):\n",
    "                    if not pd.notna(row['faq_answer']) or row['faq_answer'] == \"\":\n",
    "                        generated_question = row['generated_question']\n",
    "                        faq_answer = answerer.answer_question(generated_question)\n",
    "                        df.at[idx, 'faq_answer'] = faq_answer\n",
    "                \n",
    "                if idx % save_interval == 0:\n",
    "                    df.to_csv(output_csv, index=False)\n",
    "                    logger.info(f\"Saved progress at index {idx}\")\n",
    "                    gc.collect()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing question {idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        df.to_csv(output_csv, index=False)\n",
    "        elapsed = time.time() - start_time\n",
    "        questions_processed = batch_end - last_processed\n",
    "        avg_time_per_q = elapsed / max(1, questions_processed)\n",
    "        remaining_qs = len(df) - batch_end\n",
    "        est_time_remaining = avg_time_per_q * remaining_qs\n",
    "        \n",
    "        logger.info(f\"Processed {batch_end}/{len(df)} questions. \"\n",
    "                    f\"Avg: {avg_time_per_q:.2f}s per question. \"\n",
    "                    f\"Est. remaining: {est_time_remaining/60:.1f} minutes\")\n",
    "        gc.collect()\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed in {total_time/60:.1f} minutes. Generated answers saved to {output_csv}\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear any GPU memory that might be allocated\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def main():\n",
    "    input_file = \"T5_FAQS1.csv\" \n",
    "    output_file = \"medical_answers_finetuned_v8.csv\"\n",
    "    fine_tuned_model_dir = \"fine_tuned_medical_model_v6\"\n",
    "    \n",
    "    # Clear memory at start\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    os.makedirs(\"model_cache\", exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        gc.collect()\n",
    "        base_model_name = \"malhajar/meditron-7b-chat\"\n",
    "        \n",
    "        try:\n",
    "            model, tokenizer = load_medical_model(base_model_name)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load 7B model: {e}\")\n",
    "            logger.info(\"Falling back to smaller model...\")\n",
    "            try:\n",
    "                model, tokenizer = load_medical_model(\"microsoft/DialoGPT-medium\")\n",
    "            except Exception as e2:\n",
    "                logger.warning(f\"Failed to load DialoGPT: {e2}\")\n",
    "                logger.info(\"Falling back to even smaller model...\")\n",
    "                model, tokenizer = load_medical_model(\"distilgpt2\")\n",
    "        \n",
    "        dataset = process_dataset_for_fine_tuning(input_file)\n",
    "        if dataset:\n",
    "            model = apply_peft_to_model(model)\n",
    "            model, tokenizer = fine_tune_model(model, tokenizer, dataset, fine_tuned_model_dir)\n",
    "            \n",
    "            # Clear memory after training\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            process_questions_file(input_file, output_file, model, tokenizer)\n",
    "        else:\n",
    "            logger.error(\"Could not prepare dataset for fine-tuning. Check if the required columns exist.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main process: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "    finally:\n",
    "        clear_gpu_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting torch<3,>=2.2 (from bitsandbytes)\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vjti/.local/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/vjti/.local/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/vjti/.local/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/vjti/.local/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/vjti/.local/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/vjti/.local/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vjti/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vjti/.local/lib/python3.10/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Rolling back uninstall of nvidia-cudnn-cu12\n",
      "  Moving to /home/vjti/.local/lib/python3.10/site-packages/nvidia/__init__.py\n",
      "   from /tmp/pip-uninstall-5sym2srv/__init__.py\n",
      "  Moving to /home/vjti/.local/lib/python3.10/site-packages/nvidia/__pycache__/\n",
      "   from /home/vjti/.local/lib/python3.10/site-packages/nvidia/~_pycache__\n",
      "  Moving to /home/vjti/.local/lib/python3.10/site-packages/nvidia/cudnn/\n",
      "   from /home/vjti/.local/lib/python3.10/site-packages/nvidia/~udnn\n",
      "  Moving to /home/vjti/.local/lib/python3.10/site-packages/nvidia_cudnn_cu12-8.9.2.26.dist-info/\n",
      "   from /home/vjti/.local/lib/python3.10/site-packages/~vidia_cudnn_cu12-8.9.2.26.dist-info\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
